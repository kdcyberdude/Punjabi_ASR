{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dc048490504299a88064efd2d28265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ds: 272838, Total results: 272838; Diff in %age = 0.0\n"
     ]
    }
   ],
   "source": [
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('/mnt/pi/datasets/speech/yt_dataset')\n",
    "ds_len = len(ds['train'])\n",
    "\n",
    "f= '/home/kd/Desktop/proj/apr/Punjabi_ASR/alignment_probs_for_yt_dataset.pkl'\n",
    "\n",
    "with open(f, 'rb') as f:\n",
    "    results = plk.load(f)\n",
    "\n",
    "result_len = len(results)\n",
    "\n",
    "print(f'Total ds: {ds_len}, Total results: {result_len}; Diff in %age = {((ds_len - result_len) / ds_len) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[13]['text'] == ds['train'][results[13]['index']]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "from collections import Counter\n",
    "MIN_WORD_PROB_THRESHOLD = 0.3\n",
    "\n",
    "last_word_edge_case_count = 0\n",
    "for example in results:\n",
    "    if example['all_ds'] == None:\n",
    "        continue\n",
    "    avg_words_map = []\n",
    "    example['avg'] = copy.deepcopy(example['all_ds'])\n",
    "    if 'words_map' not in example['indicvoice_verbatim']:\n",
    "        example['indicvoice_verbatim']['words_map'] = []\n",
    "        example['all_ds']['words_map'] = []\n",
    "    try:\n",
    "        len_words = len(example['all_ds']['words_map'])\n",
    "        for idx, ((wx, _, px), (wy, _, py)) in enumerate(zip(example['indicvoice_verbatim']['words_map'], example['all_ds']['words_map'])):\n",
    "            \n",
    "            if idx == len_words - 2 and px < .10 and py > MIN_WORD_PROB_THRESHOLD:\n",
    "                    avg_words_map.append((wx, 'avg', py/2))\n",
    "                    last_word_edge_case_count += 1\n",
    "            # manual analysis weightage\n",
    "            elif py < MIN_WORD_PROB_THRESHOLD:\n",
    "                avg_words_map.append((wx, 'avg', py + (px/2)))\n",
    "            else:\n",
    "                avg_words_map.append((wx, 'avg', py))\n",
    "    except:\n",
    "        print(f'Error in example: {example[\"index\"]}')\n",
    "        raise Exception('Error')\n",
    "    example['avg']['words_map'] = avg_words_map\n",
    "\n",
    "print(f'last_word_edge_case_count: {last_word_edge_case_count}')\n",
    "def get_ins_word(words_map):\n",
    "    assert(len(words_map) > 0)\n",
    "    min_word, min_prob = words_map[0]\n",
    "    for (w, p) in words_map:\n",
    "        if p < min_prob:\n",
    "            min_word, min_prob = w, p\n",
    "    return min_word, min_prob\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "exceptions = ['ਹੈ', 'ਅ', 'ਆ', 'ਐ', 'ਹੋ','ਹੀ', 'ਹਾਂ','ਸੀ', 'ਵਿੱਚ', 'ਏ', 'ਈ', 'ਓਂ']\n",
    "def get_min_words_and_probs_within_threshold(results_filtered, key, threshold):\n",
    "    grouped_data = {}\n",
    "\n",
    "    # Initialize grouping structure with counters for each source\n",
    "    for res in results_filtered:\n",
    "        source = res['source']\n",
    "        if source not in grouped_data:\n",
    "            grouped_data[source] = {\n",
    "                'data': [],\n",
    "                'indexes': [],  # Indexes of examples for this source\n",
    "                'total_entries': 0  # Counter for total entries per source\n",
    "            }\n",
    "        \n",
    "        grouped_data[source]['total_entries'] += 1  # Increment total entries for this source\n",
    "\n",
    "        text_len = len(res['text'])\n",
    "        min_word, min_word_prob = None, 1\n",
    "        \n",
    "        for (w, _, p), (w2, _, p2) in zip(res[key]['words_map'], res['avg']['words_map']):\n",
    "            # special cases observed\n",
    "            if w in exceptions:\n",
    "                continue\n",
    "            if p < threshold and p < min_word_prob and p2 < threshold:\n",
    "                min_word_prob = p\n",
    "                min_word = w\n",
    "\n",
    "        if min_word_prob < 1:\n",
    "            grouped_data[source]['data'].append((min_word, min_word_prob))\n",
    "            grouped_data[source]['indexes'].append(res['index'])\n",
    "\n",
    "    # Process and print data for each source\n",
    "    for source, details in grouped_data.items():\n",
    "        data = details['data']\n",
    "        total_entries = details['total_entries']\n",
    "\n",
    "        insertions = (len(data) / total_entries) * 100 if total_entries else 0\n",
    "        word_lengths = [len(w) for w, p in data]\n",
    "        word_lengths_percentages = {l: (word_lengths.count(l) / len(word_lengths)) * 100 for l in set(word_lengths)}\n",
    "        \n",
    "        print(f'Source: {source}')\n",
    "        print(f'{key} Insertions: {insertions:.2f}%')\n",
    "        print('Min word length distribution:')\n",
    "        print(Counter(word_lengths))\n",
    "        print('Min word length percentage distribution:')\n",
    "        for length, percentage in word_lengths_percentages.items():\n",
    "            print(f\"Length {length}: {percentage:.2f}%\")\n",
    "        print('\\n------------------------------------------------------------------\\n')\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def get_words_and_probs_all_above_threshold(results_filtered, key, threshold):\n",
    "    grouped_data = {}\n",
    "\n",
    "    # Initialize grouping structure with counters for each source\n",
    "    for res in results_filtered:\n",
    "        source = res['source']\n",
    "        if source not in grouped_data:\n",
    "            grouped_data[source] = {\n",
    "                'data': [],\n",
    "                'indexes': [],  # Indexes of examples for this source\n",
    "                'total_entries': 0  # Counter for total entries per source\n",
    "            }\n",
    "        \n",
    "        grouped_data[source]['total_entries'] += 1  # Increment total entries for this source\n",
    "        words_above_threshold = []\n",
    "\n",
    "        # Check all words to ensure each has a probability above the threshold\n",
    "        all_above = True\n",
    "        for w, _, p in res[key]['words_map']:\n",
    "            if p <= threshold:\n",
    "                all_above = False\n",
    "                break\n",
    "            words_above_threshold.append((w, p))\n",
    "\n",
    "        # If all words are above threshold, add the data to the group\n",
    "        if all_above:\n",
    "            grouped_data[source]['data'].extend(words_above_threshold)\n",
    "            grouped_data[source]['indexes'].append(res['index'])\n",
    "\n",
    "    # Process and print data for each source\n",
    "    for source, details in grouped_data.items():\n",
    "        data = details['data']\n",
    "        total_entries = details['total_entries']\n",
    "\n",
    "        insertions = (len(data) / total_entries) * 100 if total_entries else 0\n",
    "        word_lengths = [len(w) for w, p in data]\n",
    "        word_lengths_percentages = {l: (word_lengths.count(l) / len(word_lengths)) * 100 for l in set(word_lengths)}\n",
    "        \n",
    "        print(f'Source: {source}')\n",
    "        print(f'{key} Insertions: {insertions:.2f}%')\n",
    "        print('Word length distribution:')\n",
    "        print(Counter(word_lengths))\n",
    "        print('Word length percentage distribution:')\n",
    "        for length, percentage in word_lengths_percentages.items():\n",
    "            print(f\"Length {length}: {percentage:.2f}%\")\n",
    "        print('\\n------------------------------------------------------------------\\n')\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "count_delx, count_dely, count_delavg = 0, 0, 0\n",
    "verbatim_deletion_indexes = []\n",
    "samples_removed_due_to_numeric_or_different_lang = 0\n",
    "\n",
    "for example in tqdm(results):\n",
    "    if example['all_ds'] == None:\n",
    "        samples_removed_due_to_numeric_or_different_lang += 1\n",
    "        continue\n",
    "    if example['source'] == None:\n",
    "        example['source'] = 'indicsuperb' # forgot this :;\n",
    "    delinx, deliny = False, False\n",
    "    if len(example['indicvoice_verbatim']['has_deleted_word']) > 0:\n",
    "        count_delx += 1\n",
    "        delinx = True\n",
    "        verbatim_deletion_indexes.append(example['index'])\n",
    "    if len(example['all_ds']['has_deleted_word']) > 0:\n",
    "        count_dely += 1\n",
    "        deliny = True\n",
    "    if delinx and deliny:\n",
    "        count_delavg += 1\n",
    "\n",
    "results_after_deletion_removal = [e for e in results if e['index'] not in verbatim_deletion_indexes]\n",
    "\n",
    "print(f'Percentage of samples removed due to numeric or different language: {samples_removed_due_to_numeric_or_different_lang / result_len * 100}')\n",
    "remaining_total_ds_len = ds_len - samples_removed_due_to_numeric_or_different_lang\n",
    "print(f'Percentage of examples with deleted words Verbatim: {count_delx / remaining_total_ds_len * 100}')\n",
    "print(f'Percentage of examples with deleted words Base    : {count_dely / remaining_total_ds_len * 100}')\n",
    "print(f'Percentage of examples with deleted words Average : {count_delavg / remaining_total_ds_len * 100}')\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# remove numeric or different language samples\n",
    "results_after_deletion_removal = [e for e in results if e['all_ds'] != None]\n",
    "\n",
    "# get_min_words_and_probs_within_threshold(results_after_deletion_removal, 'indicvoice_verbatim', MIN_WORD_PROB_THRESHOLD)\n",
    "# all_ds_res = get_min_words_and_probs_within_threshold(results_after_deletion_removal, 'all_ds', MIN_WORD_PROB_THRESHOLD)\n",
    "avg_res = get_min_words_and_probs_within_threshold(results_after_deletion_removal, 'avg', MIN_WORD_PROB_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset len: 272838 -> 100%\n",
      "After number, different languaage and transcript word deletion: 245061 -> 89.82\n",
      "After insertion: 159693 -> 58.53\n"
     ]
    }
   ],
   "source": [
    "insertion_indexes = []\n",
    "for source, data in avg_res.items():\n",
    "    l = data['indexes']\n",
    "    insertion_indexes.extend(l)\n",
    "\n",
    "after_insertion = len(results_after_deletion_removal) - len(insertion_indexes)\n",
    "print(f'Total Dataset len: {len(results)} -> 100%\\nAfter number, different languaage and transcript word deletion: {len(results_after_deletion_removal)} -> {len(results_after_deletion_removal)*100/len(results):.2f}\\nAfter insertion: {after_insertion} -> {after_insertion*100/len(results):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85368"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insertion_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159693\n"
     ]
    }
   ],
   "source": [
    "results_after_insertion_removal = [e for e in results_after_deletion_removal if e['index'] not in insertion_indexes]\n",
    "final_indexes = [e['index'] for e in results_after_insertion_removal]\n",
    "print(len(results_after_insertion_removal))\n",
    "\n",
    "# save as json\n",
    "import json\n",
    "with open(f'./selected_indexes_yt_dataset_{after_insertion*100/len(results):.2f}_percent.json', 'w') as f:\n",
    "    json.dump(final_indexes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sources = []\n",
    "for example in results:\n",
    "    ds_sources.append(example['source'])\n",
    "\n",
    "s = pd.Series(ds_sources)\n",
    "print(s.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4bharat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
