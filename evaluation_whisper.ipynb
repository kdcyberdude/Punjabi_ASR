{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "import speech_utils as su\n",
    "import os\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "chunk_length = 16\n",
    "\n",
    "model_name = 'openai/whisper-medium'\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name, chunk_length=chunk_length)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"Punjabi\", task=\"transcribe\")\n",
    "processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "model = WhisperForConditionalGeneration.from_pretrained('/home/kd/Desktop/proj/apr/Punjabi_ASR/checkpoints/whisper/whisper-medium-pa/checkpoint-20400')\n",
    "\n",
    "model.generation_config.language = \"punjabi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoints/whisper/whisper-medium-pa-eval\",  \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=14,\n",
    "    warmup_ratio=0.1,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_prefetch_factor=8,\n",
    "    save_total_limit=4,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    run_name='whisper-medium-pa-1-eval',\n",
    "    predict_with_generate=True,\n",
    "    save_steps=300,\n",
    "    eval_steps=300,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wers_res = []\n",
    "\n",
    "dir = '/mnt/sea/speech/benchmarks/vistaar/benchmarks/'\n",
    "dirs = os.listdir(dir)\n",
    "for d in dirs[:1]:\n",
    "    su.print_red(f'Processing {d}...')\n",
    "    manifest = f'{d}/punjabi/manifest.json' # path in manifest is {d}/punjabi/wavs/\n",
    "\n",
    "    df = pd.read_json(f'{dir}{manifest}', lines=True)\n",
    "    df['audio_filepath'] = df['audio_filepath'].apply(lambda x: f'{dir}{x}')\n",
    "    df = df.rename(columns={'audio_filepath': 'audio'})\n",
    "\n",
    "    ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    ds = ds.cast_column('audio', Audio(sampling_rate = 16000))\n",
    "    print(ds)\n",
    "    \n",
    "    ds = ds.map(prepare_dataset, num_proc=1)\n",
    "\n",
    "    # TODO: Handle this case of long sequences\n",
    "    lengths = ds['labels']\n",
    "    lengths = [len(i) for i in lengths]\n",
    "    selected_indexes = [i for i in range(len(lengths)) if lengths[i] < 448 ] \n",
    "    su.print_red(f\"Removing {len(ds) - len(selected_indexes)} samples from this benchmark\")\n",
    "    ds = ds.select(selected_indexes)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        eval_dataset=ds,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    wer = trainer.evaluate(eval_dataset=ds)['eval_wer']\n",
    "    print(f'WER of {d}: {wer}')\n",
    "    wers_res.append((d, f'{wer}:.2f'))\n",
    "\n",
    "\n",
    "print(wers_res)\n",
    "s = 0\n",
    "for i in wers_res:\n",
    "    s += i[1]\n",
    "print(f'Average WER: {sum/len(wers_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vistar benchmark\n",
    "[('commonvoice', 0.224), ('fleurs', 0.231), ('kathbath', 0.169), ('kathbath_noisy', 0.197)]\n",
    "\n",
    "# WHISPER\n",
    "\n",
    "# /home/kd/Desktop/proj/apr/Punjabi_ASR/checkpoints/whisper/whisper-medium-pa/checkpoint-20400\n",
    "[('commonvoice', 21.604938271604937), ('fleurs', 24.047335516522775), ('kathbath', 23.58632650266989), ('kathbath_noisy', 26.694354433846012)]\n",
    "\n",
    "\n",
    "\n",
    "print('-----WER-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
