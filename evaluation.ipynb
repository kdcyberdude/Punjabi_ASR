{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/Desktop/proj/apr/Punjabi_ASR/speech_utils.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n",
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/kd/.cache/pyctcdecode/models--kdcyberdude--w2v-bert-punjabi/snapshots/242700f010dcc5181e31b66b3ef0965a3c1aa307/language_model/5gram_pa_3_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\"Processing commonvoice...\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07b9533b5a4295be78e7f9d8acbd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac41db0621545919df40339216ac22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER of commonvoice: 0.13503086419753085\n",
      "\u001b[31m\"Processing fleurs...\"\u001b[0m\n",
      "Removed 104 sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e761ef4186e465fbc4688c3b5db1dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f43da2ce744cea6b2b154a4068821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER of fleurs: 0.11489810260014055\n",
      "\u001b[31m\"Processing kathbath...\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6324136f77774eb3a05d2a4cffd5395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f480c92ae344093b2a88fbffce9db2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER of kathbath: 0.11317567567567567\n",
      "\u001b[31m\"Processing kathbath_noisy...\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89549a3e4b394dc5b783ea8d1c8925e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/ai4bharat/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb81f99647d4241bc0a28a22650741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER of kathbath_noisy: 0.12367604090577064\n",
      "[('commonvoice', 0.13503086419753085), ('fleurs', 0.11489810260014055), ('kathbath', 0.11317567567567567), ('kathbath_noisy', 0.12367604090577064)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, Audio\n",
    "import speech_utils as su\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import SeamlessM4TFeatureExtractor\n",
    "from transformers import Wav2Vec2BertProcessor\n",
    "from transformers import Wav2Vec2BertForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from data_collator import DataCollatorCTCWithPadding\n",
    "\n",
    "\n",
    "processor_with_lm_path = 'kdcyberdude/w2v-bert-punjabi'\n",
    "# Uncomment below line if you want to evaluate on benchmarks without LM decoder\n",
    "# processor_with_lm_path = None\n",
    "dir = '/mnt/sea/speech/benchmarks/vistaar/benchmarks/'\n",
    "model_path = '/home/kd/Desktop/proj/apr/Punjabi_ASR/checkpoints/wav2vec2-bert-pa_indicvoice_verbatim_2/checkpoint-3500'\n",
    "tokenizer_path = './'\n",
    "dirs = os.listdir(dir)\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(tokenizer_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(model_path)\n",
    "\n",
    "for_lm = False\n",
    "if processor_with_lm_path is not None:\n",
    "    from m4t_processor_with_lm import M4TProcessorWithLM\n",
    "    processor = M4TProcessorWithLM.from_pretrained(processor_with_lm_path)\n",
    "    for_lm = True\n",
    "else:\n",
    "    processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "model = Wav2Vec2BertForCTC.from_pretrained(\n",
    "    model_path, ignore_mismatched_sizes=True,\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    add_adapter=True,\n",
    "    use_intermediate_ffn_before_adapter=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "accumulation_steps = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./benchmark_runs/\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=accumulation_steps,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_prefetch_factor=8,\n",
    "    dataloader_persistent_workers=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    " \n",
    "\n",
    "wers_res = []\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "vocab_chars = list(tokenizer.get_vocab().keys())[:-5]\n",
    "vocab_chars.append(' ')\n",
    "def compute_wer_metrics(pred):\n",
    "\n",
    "    pred_logits = pred.predictions\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    if processor_with_lm_path is None:\n",
    "        # needed for wihout LM decoding\n",
    "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids)\n",
    "        # we do not want to group tokens when computing the metrics\n",
    "        label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    else:\n",
    "        # we do not want to group tokens when computing the metrics\n",
    "        label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "        pred_str = processor.batch_decode(pred_logits).text\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "for d in dirs[:]:\n",
    "    su.print_red(f'Processing {d}...')\n",
    "    manifest = f'{d}/punjabi/manifest.json' # path in manifest is {d}/punjabi/wavs/\n",
    "\n",
    "    df = pd.read_json(f'{dir}{manifest}', lines=True)\n",
    "    df['audio_filepath'] = df['audio_filepath'].apply(lambda x: f'{dir}{x}')\n",
    "    df = df.rename(columns={'audio_filepath': 'audio'})\n",
    "\n",
    "    ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    ds = ds.cast_column('audio', Audio(sampling_rate = 16000))\n",
    "    \n",
    "    ds = su.normalize_texts_for_inference(ds, vocab_chars, strategy='remove')\n",
    "    ds = ds.map(lambda batch: su.process_dataset(batch, processor), remove_columns=ds.column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_wer_metrics,\n",
    "        eval_dataset=ds,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    wer = trainer.evaluate(eval_dataset=ds)['eval_wer']\n",
    "    print(f'WER of {d}: {wer}')\n",
    "    wers_res.append((d, wer))\n",
    "\n",
    "\n",
    "print(wers_res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/mnt/sea/speech/'\n",
    "# d = 'indictts_ds'\n",
    "# su.print_red(f'Processing {d}...')\n",
    "# manifest = f'{d}/punjabi/manifest.json' # path in manifest is {d}/punjabi/wavs/\n",
    "\n",
    "# df = pd.read_json(f'{dir}{manifest}', lines=True)\n",
    "# df['audio_filepath'] = df['audio_filepath'].apply(lambda x: f'{dir}{x}')\n",
    "# df = df.rename(columns={'audio_filepath': 'audio'})\n",
    "\n",
    "# ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "# ds = ds.cast_column('audio', Audio(sampling_rate = 16000))\n",
    "# ds = su.add_silence(ds)\n",
    "\n",
    "# vocab_chars = list(tokenizer.get_vocab().keys())[1:-5]\n",
    "# ds = su.normalize_texts_for_inference(ds, vocab_chars)\n",
    "# ds = su.remove_text_samples(ds, column_name='normalized_text')\n",
    "# ds = ds.map(lambda batch: su.process_dataset(batch, processor), remove_columns=ds.column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_wer_metrics,\n",
    "#     eval_dataset=ds,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "# )\n",
    "# wer = trainer.evaluate(eval_dataset=ds)['eval_wer']\n",
    "# print(f'WER of {d}: {wer}')\n",
    "# wers_res.append((d, wer))\n",
    "\n",
    "\n",
    "# print(wers_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----WER-----\n"
     ]
    }
   ],
   "source": [
    "# vistar benchmark\n",
    "[('commonvoice', 0.224), ('fleurs', 0.231), ('kathbath', 0.169), ('kathbath_noisy', 0.197)]\n",
    "\n",
    "# at checkpoint 6900\n",
    "[('commonvoice', 0.19135802469135801), ('fleurs', 0.1991391426563598), ('kathbath', 0.17808406736342478), ('kathbath_noisy', 0.1969330473278262)]\n",
    "\n",
    "# at checkpoint 6900 with LM decoding (Vocab with vowels) - FIRST \n",
    "[('commonvoice', 0.1111111111111111), ('fleurs', 0.06175333801827126), ('kathbath', 0.07343343526082789), ('kathbath_noisy', 0.08584729131486468)]\n",
    "\n",
    "# checkpoint 12000\n",
    "[('commonvoice', 0.22376543209876543), ('fleurs', 0.19246310611384398), ('kathbath', 0.17904249007347908), ('kathbath_noisy', 0.19780019168454202)]\n",
    "\n",
    "# checkpoint 7500 previous data\n",
    "[('commonvoice', 0.24228395061728394), ('fleurs', 0.2122276879831342), ('kathbath', 0.2253206152161015), ('kathbath_noisy', 0.2402446259869472)]\n",
    "\n",
    "# checkpoint wav2vec2-bert-pa_4/checkpoint-12300\n",
    "[('commonvoice', 0.19521604938271606), ('fleurs', 0.18587491215741392), ('kathbath', 0.170005933092967), ('kathbath_noisy', 0.18716626352060609)]\n",
    "\n",
    "# checkpoint wav2vec2-bert-pa_4/checkpoint-12300 - text normalization function changes\n",
    "[('commonvoice', 0.19444444444444445), ('fleurs', 0.18640196767392833), ('kathbath', 0.16996895544192842), ('kathbath_noisy', 0.18672388604821039), ('indictts_ds', 0.3322171662399517)]\n",
    "\n",
    "# checkpoint wav2vec2-bert-pa_4/checkpoint-12300 - text normalization function changes with lm (Vocab with no vowels) - SECOND\n",
    "[('commonvoice', 0.10802469135802469), ('fleurs', 0.06553056921995784), ('kathbath', 0.07432432432432433), ('kathbath_noisy', 0.09085098612125639), ('indictts_ds', 0.32349093836297976)]\n",
    "\n",
    "# checkpoint wav2vec2-bert-pa_4/checkpoint-12300 with LM - trained on stories and wiki\n",
    "[('commonvoice', 0.1697530864197531), ('fleurs', 0.13729796205200281), ('kathbath', 0.12413257852447042), ('kathbath_noisy', 0.14024835646457268), ('indictts_ds', 0.3242511990876871)]\n",
    "\n",
    "# above with strategy num2word\n",
    "(0, 104, 0, 0) # number of numeric samples converted\n",
    "[('commonvoice', 0.1697530864197531), ('fleurs', 0.15446811502495958), ('kathbath', 0.12432111724704487), ('kathbath_noisy', 0.14043174661129113)]\n",
    "\n",
    "# above with not removing non-vocab characters\n",
    "[('commonvoice', 0.1705246913580247), ('fleurs', 0.2245491977850348), ('kathbath', 0.12432111724704487), ('kathbath_noisy', 0.14043174661129113)]\n",
    "# above + fluers with strategy remove -\n",
    "[('fleurs', 0.208237378210806)] # characters not presented in vocab - ['-', ':', '?', 'n', '.', '6', \"'\", '8', 'r', 'h', '2', '1', 'i', '3', 'Â¾', '7', 'Â½', '4', 'â€˜', '5', 'f', '~', 'c', '$', 'à¥¤', '0', '\"', 'o', 'g', 'l', 's', 'â€', '9', 'a', 'e', ';', '/'] - same without numbers - ['-', ':', '?', 'n', '.', 'r', \"'\", 'h', 'i', 'Â¾', 'Â½', 'â€˜', 'f', '~', 'c', '$', 'à¥¤', '\"', 'o', 'g', 'l', 's', 'â€', 'a', 'e', ';', '/']\n",
    "\n",
    "# further fine tune based model with clean data sources and vocabulary extension - removed ds are shrutilipi, cmu & google synth. with no LM - /home/kd/Desktop/proj/apr/speech_pa/wav2vec2-bert-pa_5/checkpoint-8400\n",
    "[('commonvoice', 0.18209876543209877), ('fleurs', 0.15144061841180603), ('kathbath', 0.15362490869247625), ('kathbath_noisy', 0.1681428049671293)]\n",
    "\n",
    "# above with - /home/kd/Desktop/proj/apr/speech_pa/wav2vec2-bert-pa_5/checkpoint-1800\n",
    "[('commonvoice', 0.21141975308641975), ('fleurs', 0.19167252283907238), ('kathbath', 0.19133491599707816), ('kathbath_noisy', 0.2114682249817385)]\n",
    "\n",
    "# wav2vec2-bert-pa_5/checkpoint-1800 with LM(all data)\n",
    "[('commonvoice', 0.11805555555555555), ('fleurs', 0.07290934645115953), ('kathbath', 0.08391161431701973), ('kathbath_noisy', 0.09893170197224252)]\n",
    "\n",
    "# wav2vec2-bert-pa_5/checkpoint-1800 with LM(all data) - do not removed out of vocab characters\n",
    "[('commonvoice', 0.11496913580246913), ('fleurs', 0.07782853127196064), ('kathbath', 0.08213111760409057), ('kathbath_noisy', 0.095827246165084)]\n",
    "\n",
    "# wav2vec2-bert-pa_5/checkpoint-1800 with LM - On Wiki And ASR DS - do not removed out of vocab characters on Training LM (Vocab with vowels) - THIRD\n",
    "[('commonvoice', 0.10185185185185185), ('fleurs', 0.06728742094167252), ('kathbath', 0.08222242512783054), ('kathbath_noisy', 0.091672753834916), ('indictts_ds', 0.26857328130764846)]\n",
    "\n",
    "# /home/kd/Desktop/proj/apr/speech_pa/wav2vec2-bert-pa_5/checkpoint-4000 - above LM wav2vec2-bert-pa-lm-processor-all_3 - FOURTH\n",
    "[('commonvoice', 0.10185185185185185), ('fleurs', 0.06965917076598735), ('kathbath', 0.08308984660336012), ('kathbath_noisy', 0.0931336742147553)]\n",
    "\n",
    "# wav2vec2-bert-pa_5/checkpoint-1800 with LM - On Wiki And ASR DS - do not removed out of vocab characters on Training LM (Vocab with vowels) - wav2vec2-bert-pa-lm-processor-all_3 - strategy - nothing\n",
    "[('commonvoice', 0.11574074074074074), ('fleurs', 0.10082704063286588), ('kathbath', 0.08464207450693938), ('kathbath_noisy', 0.09888604821037253)]\n",
    "\n",
    "# wav2vec2-bert-pa_5/checkpoint-1800 with LM - On Wiki And ASR DS - do not removed out of vocab characters on Training LM (Vocab with vowels) - wav2vec2-bert-pa-lm-processor-all_3 - strategy - remove\n",
    "[('commonvoice', 0.11574074074074074), ('fleurs', 0.07703794799718904), ('kathbath', 0.08464207450693938), ('kathbath_noisy', 0.09888604821037253)]\n",
    "\n",
    "\n",
    "# IndicVoice Verbatim trained for 2 epochs checkpoint - checkpoints/wav2vec2-bert-pa_indicvoice_verbatim/checkpoint-3000\n",
    "[('commonvoice', 0.13040123456790123), ('fleurs', 0.11226282501756851), ('kathbath', 0.1039536157779401), ('kathbath_noisy', 0.11641709276844411)]\n",
    "\n",
    "\n",
    "[('commonvoice', 0.13503086419753085), ('fleurs', 0.11489810260014055), ('kathbath', 0.11317567567567567), ('kathbath_noisy', 0.12367604090577064)]\n",
    "\n",
    "\n",
    "print('-----WER-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('à¨à¨')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
