{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "datasets_dirs = [\n",
    "    '/mnt/sea/speech/processed_datasets/Common_Voice_16_1_pa-IN_ASR',\n",
    "    '/mnt/sea/speech/processed_datasets/fleurs_pa_ASR',\n",
    "    '/mnt/sea/speech/ds_filtered/indicsuperb_pa_ASR_filtered',\n",
    "    '/mnt/sea/speech/processed_datasets/Indicvoice_pa_ASR',\n",
    "    '/mnt/sea/speech/ds_filtered/shrutilipi_pa_ASR_filtered' \n",
    "]\n",
    "\n",
    "# cosmopedia_shards = ['/mnt/sea/it2_translation_shards/stories/translated_0', '/mnt/sea/it2_translation_shards/web_samples_v1/translated_0', '/mnt/sea/it2_translation_shards/web_samples_v2/translated_0']\n",
    "\n",
    "indictts_manifest_json = '/mnt/sea/speech/indictts_ds/punjabi/manifest.json'\n",
    "wikipedia = '/mnt/sea/speech/kenlm_wikipedia/train-00000-of-00001.parquet'\n",
    "\n",
    "wiki_ds = Dataset.from_parquet(wikipedia)\n",
    "\n",
    "\n",
    "df = pd.read_json(f'{indictts_manifest_json}', lines=True)\n",
    "indic_ds = Dataset.from_pandas(df)\n",
    "\n",
    "dss = []\n",
    "# for d in cosmopedia_shards:\n",
    "#     try:\n",
    "#         ds = load_from_disk(f'{d}')\n",
    "#         dss.append(ds)\n",
    "#     except Exception as e:\n",
    "#         print(d)\n",
    "\n",
    "all_data_splits = []\n",
    "for d in datasets_dirs:\n",
    "    ds = load_from_disk(f'{d}')\n",
    "    for split in ds:\n",
    "        all_data_splits.append(ds[split])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['speaker_id', 'audio', 'text', 'gender', 'duration'],\n",
      "    num_rows: 166\n",
      "}), Dataset({\n",
      "    features: ['speaker_id', 'audio', 'text', 'gender', 'duration'],\n",
      "    num_rows: 487\n",
      "}), Dataset({\n",
      "    features: ['speaker_id', 'audio', 'text', 'gender', 'duration'],\n",
      "    num_rows: 286\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'gender', 'duration', 'speaker_id'],\n",
      "    num_rows: 1923\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'gender', 'duration', 'speaker_id'],\n",
      "    num_rows: 251\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'gender', 'duration', 'speaker_id'],\n",
      "    num_rows: 574\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration'],\n",
      "    num_rows: 22268\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration'],\n",
      "    num_rows: 7468\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration'],\n",
      "    num_rows: 17300\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration'],\n",
      "    num_rows: 95\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'gender', 'speaker_id', 'duration'],\n",
      "    num_rows: 27991\n",
      "}), Dataset({\n",
      "    features: ['audio', 'text', 'gender', 'speaker_id', 'duration'],\n",
      "    num_rows: 1159\n",
      "})]\n",
      "Dataset({\n",
      "    features: ['audio_filepath', 'duration', 'text'],\n",
      "    num_rows: 14527\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 51423\n",
      "})\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(all_data_splits)\n",
    "print(indic_ds)\n",
    "print(wiki_ds)\n",
    "print(dss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51423\n",
      "65950\n",
      "145918\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# get text from all dataset splits and create a list\n",
    "all_texts_list = []\n",
    "for d in tqdm(dss):\n",
    "    all_texts_list.extend(d['translated'])\n",
    "print(len(all_texts_list))\n",
    "all_texts_list.extend(wiki_ds['text'])\n",
    "print(len(all_texts_list))\n",
    "all_texts_list.extend(indic_ds['text'])\n",
    "print(len(all_texts_list))\n",
    "for d in all_data_splits:\n",
    "    all_texts_list.extend(d['text'])\n",
    "print(len(all_texts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/Desktop/proj/apr/speech_pa/speech_utils.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n",
      "/home/kd/anaconda3/envs/bharat/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/kd/anaconda3/envs/bharat/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/kd/anaconda3/envs/bharat/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡®Ç', '‡®Ö', '‡®Ü', '‡®á', '‡®à', '‡®â', '‡®ä', '‡®è', '‡®ê', '‡®ì', '‡®î', '‡®ï', '‡®ñ', '‡®ó', '‡®ò', '‡®ô', '‡®ö', '‡®õ', '‡®ú', '‡®ù', '‡®û', '‡®ü', '‡®†', '‡®°', '‡®¢', '‡®£', '‡®§', '‡®•', '‡®¶', '‡®ß', '‡®®', '‡®™', '‡®´', '‡®¨', '‡®≠', '‡®Æ', '‡®Ø', '‡®∞', '‡®≤', '‡®≥', '‡®µ', '‡®∂', '‡®∏', '‡®π', '‡®º', '‡®æ', '‡®ø', '‡©Ä', '‡©Å', '‡©Ç', '‡©á', '‡©à', '‡©ã', '‡©å', '‡©ç', '‡©ô', '‡©ö', '‡©õ', '‡©ú', '‡©û', '‡©∞', '‡©±', '‡©≤', '‡©≥', ' ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145918/145918 [00:08<00:00, 17622.86it/s] \n"
     ]
    }
   ],
   "source": [
    "# config params\n",
    "base_dir = '/home/kd/Desktop/proj/apr/speech_pa/'\n",
    "vocab_has_vowels = True\n",
    "convert_num_to_words = False\n",
    "tokenizer_path = './'\n",
    "ds_txt_file_name = f\"{base_dir}kenlm_pa_ds_3.txt\"\n",
    "kenlm_train_txt_file = f'{base_dir}kenlm_pa_ds_3.txt'\n",
    "kenlm_output_arpa_file = f'{base_dir}5gram_pa_3.arpa'\n",
    "kenlm_output_correct_arpa_file = kenlm_output_arpa_file.replace('.arpa', '_correct.arpa')\n",
    "kenlm_output_correct_bin_file = kenlm_output_correct_arpa_file.replace('.arpa', '.bin')\n",
    "processor_with_lm_save_path = f\"{base_dir}/wav2vec2-bert-pa-lm-processor-all_3\"\n",
    "\n",
    "import speech_utils as su \n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(tokenizer_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "chars = sorted_vocab_dict.keys()\n",
    "remove_chars = ['|', '[unk]', '[pad]', '<s>', '</s>']\n",
    "chars = [c for c in chars if c not in remove_chars]\n",
    "chars.append(' ')\n",
    "print(chars)\n",
    "\n",
    "# Required if we trained the wav2vec-bert with no vowels vocab\n",
    "replacements = {\n",
    "    '‡®Ü': '‡®Ö‡®æ',\n",
    "    '‡®á': '‡®ø‡©≤',\n",
    "    '‡®à': '‡©≤‡©Ä',\n",
    "    '‡®â': '‡©≥‡©Å',\n",
    "    '‡®ä' :'‡©≥‡©Ç',\n",
    "    '‡®è': '‡©≤‡©á',\n",
    "    '‡®ê': '‡®Ö‡©à',\n",
    "    '‡®î': '‡®Ö‡©å',\n",
    "}\n",
    "texts = []\n",
    "for text in tqdm(all_texts_list):\n",
    "    if convert_num_to_words:\n",
    "        text = su.num_to_words(text)\n",
    "\n",
    "    if not vocab_has_vowels:\n",
    "        for k, v in replacements.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ds_txt_file_name, \"w\", buffering=8192) as file:\n",
    "    for text in texts:\n",
    "        file.write(text + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡®Æ‡©∞‡®§‡®∞‡©Ä ‡®®‡©á ‡®ï‡®ø‡®π‡®æ ‡®ï‡®ø ‡®¶‡®ø‡©±‡®≤‡©Ä ‡®Æ‡©Å‡©∞‡®¨‡®à ‡®Æ‡©Å‡©±‡®ñ ‡®Æ‡®æ‡®∞‡®ó ‡®§‡©á ‡®â‡®®‡©ç‡®π‡®æ‡®Ç ‡®¶‡®æ ‡®Æ‡©∞‡®§‡®∞‡®æ‡®≤‡®æ ‡®¨‡®ø‡®ú‡®≤‡©Ä ‡®®‡®æ‡®≤ ‡®ö‡©±‡®≤‡®£ ‡®µ‡®æ‡®≤‡©á ‡®µ‡®æ‡®π‡®®‡®æ‡®Ç ‡®≤‡®à ‡®¨‡©Å‡®®‡®ø‡®Ü‡®¶‡©Ä ‡®¢‡®æ‡®Ç‡®ö‡©á ‡®®‡©Ç‡©∞ ‡®â‡®§‡®∏‡®º‡®æ‡®π‡®ø‡®§ ‡®ï‡®∞‡®® ‡®≤‡®à ‡®ö‡®æ‡®∞‡®ú‡®ø‡©∞‡®ó ‡®∏‡®ü‡©á‡®∏‡®º‡®® ‡®ê‡®≤ ‡®ê‡®® ‡®ú‡©Ä ‡®∏‡®ü‡©á‡®∏‡®º‡®®‡®æ‡®Ç ‡®§‡©á ‡®™‡©Ç‡©∞‡®ú‡©Ä ‡®®‡®ø‡®µ‡©á‡®∏‡®º ‡®ï‡®∞‡©á‡®ó‡®æ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/kd/Desktop/proj/apr/speech_pa/kenlm_pa_ds_3.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 18221306 types 802665\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:9631980 2:5898565120 3:11059810304 4:17695694848 5:25806223360\n",
      "Statistics:\n",
      "1 802664 D1=0.75451 D2=1.04411 D3+=1.26913\n",
      "2 5317881 D1=0.815085 D2=1.08912 D3+=1.29471\n",
      "3 11451226 D1=0.889659 D2=1.1995 D3+=1.34996\n",
      "4 15044514 D1=0.944361 D2=1.31942 D3+=1.39561\n",
      "5 16559530 D1=0.916065 D2=1.67761 D3+=1.39807\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 1032 assuming -p 1.5\n",
      "probing 1217 assuming -r models -p 1.5\n",
      "trie     524 without quantization\n",
      "trie     301 assuming -q 8 -b 8 quantization \n",
      "trie     461 assuming -a 22 array pointer compression\n",
      "trie     238 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:9631968 2:85086096 3:229024520 4:361068336 5:463666840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:9631968 2:85086096 3:229024520 4:361068336 5:463666840\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:59224540 kB\tVmRSS:27036 kB\tRSSMax:10896848 kB\tuser:23.152\tsys:8.41926\tCPU:31.5712\treal:22.9556\n"
     ]
    }
   ],
   "source": [
    "!/mnt/sea/proj/apr/kenlm/build/bin/lmplz -o 5 < {kenlm_train_txt_file} > {kenlm_output_arpa_file} -S 90% --skip_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kenlm_output_arpa_file, \"r\") as read_file, open(kenlm_output_correct_arpa_file, \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in  read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/mnt/sea/proj/apr/kenlm/build/bin/build_binary {kenlm_output_correct_arpa_file} {kenlm_output_correct_bin_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/bharat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/kd/Desktop/proj/apr/speech_pa/5gram_pa_3_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import SeamlessM4TFeatureExtractor\n",
    "from transformers import Wav2Vec2BertProcessor\n",
    "from transformers import Wav2Vec2BertForCTC\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(tokenizer_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=kenlm_output_correct_bin_file,\n",
    ")\n",
    "\n",
    "from m4t_processor_with_lm import M4TProcessorWithLM\n",
    "\n",
    "processor_with_lm = M4TProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")\n",
    "\n",
    "processor_with_lm.save_pretrained(processor_with_lm_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
