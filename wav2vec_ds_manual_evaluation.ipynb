{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_from_disk, concatenate_datasets, DatasetDict, Dataset\n",
    "import IPython.display as ipd\n",
    "import speech_utils as su\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import SeamlessM4TFeatureExtractor\n",
    "from transformers import Wav2Vec2BertProcessor\n",
    "from transformers import Wav2Vec2BertForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_metric, Audio\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "datasets_dir = [\n",
    "    '/mnt/sea/speech/processed_datasets/CMU_Synth_ASR',\n",
    "      '/mnt/sea/speech/processed_datasets/Common_Voice_16_1_pa-IN_ASR', \n",
    "      '/mnt/sea/speech/processed_datasets/fleurs_pa_ASR',\n",
    "      '/mnt/sea/speech/processed_datasets/Google_Synth_ASR',\n",
    "       '/mnt/sea/speech/processed_datasets/IndicSuperb_pa_ASR',\n",
    "       '/mnt/sea/speech/processed_datasets/Indicvoice_pa_ASR',\n",
    "       '/mnt/sea/speech/processed_datasets/PunjabiSpeech_A_labeled_Speech_Corpus_ASR',\n",
    "        '/mnt/sea/speech/processed_datasets/shrutilipi_pa_ASR'\n",
    "]\n",
    "datasets = [load_from_disk(f'{d}') for d in datasets_dir]\n",
    "\n",
    "all_data_splits = []\n",
    "train_data_splits = []\n",
    "train_valid_data_splits = []\n",
    "test_data_splits = []\n",
    "valid_data_splits = []\n",
    "\n",
    "\n",
    "train_splits_log = []\n",
    "test_splits_log = []\n",
    "for d, ds_dir in zip(datasets, datasets_dir):\n",
    "    for split in d:\n",
    "        all_data_splits.append(d[split])\n",
    "        if split == 'train':\n",
    "            train_data_splits.append(d[split])\n",
    "        if split == 'train' or 'valid' in split:\n",
    "            train_splits_log.append(f'{ds_dir.split('/')[-1]} - {split}')\n",
    "            train_valid_data_splits.append(d[split])\n",
    "        if split == 'test':\n",
    "            test_splits_log.append(f'{ds_dir.split('/')[-1]} - {split}')\n",
    "            test_data_splits.append(d[split])\n",
    "        if 'valid' in split:\n",
    "            valid_data_splits.append(d[split])\n",
    "\n",
    "print(len(all_data_splits))\n",
    "print(len(train_data_splits))\n",
    "print(len(train_valid_data_splits))\n",
    "print(len(test_data_splits))\n",
    "print(len(valid_data_splits))\n",
    "\n",
    "print(\"Train Data Splits\")\n",
    "print(train_splits_log)\n",
    "print(\"Test Data Splits\")\n",
    "print(test_splits_log)\n",
    "\n",
    "\n",
    "ds_all = concatenate_datasets(all_data_splits)\n",
    "ds_train = concatenate_datasets(train_data_splits)\n",
    "ds_train_valid = concatenate_datasets(train_valid_data_splits)\n",
    "ds_test = concatenate_datasets(test_data_splits)\n",
    "ds_valid = concatenate_datasets(valid_data_splits)\n",
    "\n",
    "print(ds_all)\n",
    "print(ds_train)\n",
    "print(ds_train_valid)\n",
    "print(ds_test)\n",
    "print(ds_valid)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': ds_train_valid,\n",
    "    'test': ds_test,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDICTTS TEST\n",
    "dir = ['/mnt/sea/speech/', '/mnt/sea/speech/benchmarks/vistaar/benchmarks/']\n",
    "d = ['indictts_ds', 'fleurs']\n",
    "\n",
    "dir = dir[1]\n",
    "d = d[1]\n",
    "\n",
    "su.print_red(f'Processing {d}...')\n",
    "manifest = f'{d}/punjabi/manifest.json' # path in manifest is {d}/punjabi/wavs/\n",
    "df = pd.read_json(f'{dir}{manifest}', lines=True)\n",
    "df['audio_filepath'] = df['audio_filepath'].apply(lambda x: f'{dir}{x}')\n",
    "df = df.rename(columns={'audio_filepath': 'audio'})\n",
    "ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "ds = ds.cast_column('audio', Audio(sampling_rate = 16000))\n",
    "if 'test' not in ds.column_names:\n",
    "    ds = ds.train_test_split(test_size=0.01, seed=42)\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_from_disk('/mnt/sea/speech/processed_datasets/IndicSuperb_pa_ASR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su.get_summary(ds)\n",
    "\n",
    "# if train and valid splits are there - change name of valid split to test\n",
    "if 'noisy_test' in ds.column_names:\n",
    "    train = concatenate_datasets([ds['train'], ds['valid']])\n",
    "    test = concatenate_datasets([ds['test'], ds['test_known'], ds['noisy_test'], ds['noisy_test_known']])\n",
    "    ds = DatasetDict({'train': train, 'test': test})\n",
    "\n",
    "if 'valid' in ds.column_names and 'test' not in ds.column_names:\n",
    "    ds['test'] = ds['valid']\n",
    "    del ds['valid']\n",
    "\n",
    "# split ['train'] to ['train', 'test']\n",
    "if 'test' not in ds.column_names:\n",
    "    ds = ds['train'].train_test_split(test_size=0.04, seed=42)\n",
    "    print(ds)\n",
    "\n",
    "if 'valid' in ds.column_names:\n",
    "    ds = su.merge_train_valid_splits(ds)\n",
    "\n",
    "ds = su.add_silence(ds)\n",
    "ds['train'] = su.remove_audio_samples(ds['train'])\n",
    "ds['test'] = su.remove_audio_samples(ds['test'])\n",
    "ds = su.normalize_text_ds(ds)\n",
    "ds['train'] = su.remove_text_samples(ds['train'], column_name='normalized_text')\n",
    "ds['test'] = su.remove_text_samples(ds['test'], column_name='normalized_text')\n",
    "su.get_summary(ds, text_column='normalized_text')\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # batch is single row\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    clean_audio_arr = audio[\"array\"]\n",
    "    noised_audio = clean_audio_arr\n",
    "\n",
    "    batch[\"input_features\"] = processor(noised_audio, sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_features\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"normalized_text\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_wer = False # if True, calculate WER for each audio file\n",
    "\n",
    "eval_split = 'train'\n",
    "ckpt = 'wav2vec2-bert-pa_4/checkpoint-12300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import compute_measures\n",
    "import datasets as datasets_lib\n",
    "class CustomWER(datasets_lib.Metric):\n",
    "    def _info(self):\n",
    "        return datasets_lib.MetricInfo(\n",
    "            description='_DESCRIPTION',\n",
    "            citation='_CITATION',\n",
    "            inputs_description='_KWARGS_DESCRIPTION',\n",
    "            features=datasets_lib.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets_lib.Value(\"string\", id=\"sequence\"),\n",
    "                    \"references\": datasets_lib.Value(\"string\", id=\"sequence\"),\n",
    "                }\n",
    "            ),\n",
    "            codebase_urls=[\"https://github.com/jitsi/jiwer/\"],\n",
    "            reference_urls=[\n",
    "                \"https://en.wikipedia.org/wiki/Word_error_rate\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions=None, references=None, concatenate_texts=False):\n",
    "        wers = []\n",
    "        for prediction, reference in zip(predictions, references):\n",
    "            incorrect = 0\n",
    "            total = 0\n",
    "            try:\n",
    "                measures = compute_measures(reference, prediction)\n",
    "                incorrect += measures[\"substitutions\"] + measures[\"deletions\"] + measures[\"insertions\"]\n",
    "                total += measures[\"substitutions\"] + measures[\"deletions\"] + measures[\"hits\"]\n",
    "                wers.append(incorrect / total)\n",
    "            except Exception as e:\n",
    "                wers.append(1000.0)\n",
    "                print(f\"Error in WER calculation for {prediction} and {reference}\")\n",
    "\n",
    "        \n",
    "        # save wers to file - For testing purposes\n",
    "        with open(f\"{eval_split}_wers.txt\", \"a\") as f:\n",
    "            f.write(f\"{wers}\\n\")\n",
    "        return wers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429a55a19c06475aaaf7e00ddf146e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/envs/hf2/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for manual evaluation/testing of specific split\n",
    "if eval_split == 'test':\n",
    "    dsp_train = ds['test'].map(prepare_dataset, remove_columns=ds['test'].column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "    dsp_test = dsp_train\n",
    "elif eval_split == 'train':\n",
    "    dsp_train = ds['train'].map(prepare_dataset, remove_columns=ds['train'].column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "    dsp_test = dsp_train\n",
    "else:\n",
    "    dsp_train = ds['train'].map(prepare_dataset, remove_columns=ds['train'].column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "    dsp_test = ds['test'].map(prepare_dataset, remove_columns=ds['test'].column_names, num_proc=1, batch_size=64, writer_batch_size=64, )\n",
    "\n",
    "\n",
    "dsp = DatasetDict({\n",
    "    'train': dsp_train,\n",
    "    'test': dsp_test,\n",
    "})\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2BertProcessor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "if individual_wer:\n",
    "    wer_metric = CustomWER()\n",
    "else:\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "\n",
    "\n",
    "model = Wav2Vec2BertForCTC.from_pretrained(\n",
    "    ckpt,\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    add_adapter=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "accumulation_steps = 1\n",
    "\n",
    "effective_batch_size = batch_size * accumulation_steps\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./wav2vec2-bert-pa_eval',\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  gradient_accumulation_steps=accumulation_steps,\n",
    "  per_device_eval_batch_size=6,\n",
    "  eval_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  adam_beta1=0.9,\n",
    "  adam_beta2=0.999,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=5,\n",
    "  learning_rate=5e-5,\n",
    "  lr_scheduler_type=\"cosine\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"wer\",\n",
    "  greater_is_better=False,\n",
    "  ignore_data_skip=True,\n",
    "  save_total_limit=4,\n",
    "  push_to_hub=False,\n",
    "  report_to= \"none\",\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=partial(su.compute_wer_metrics, processor=processor),\n",
    "    train_dataset=dsp['train'],\n",
    "    eval_dataset=dsp['test'],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range = 465\n",
    "dspe = dsp[eval_split]\n",
    "dse = ds[eval_split]\n",
    "\n",
    "if test_range != 0:\n",
    "    if test_range > 0:\n",
    "        dspe = dspe.select(range(0, test_range))\n",
    "        dse = dse.select(range(0, test_range))\n",
    "    else:\n",
    "        dspe = dspe.select(range(len(dspe) + test_range, len(dspe)))\n",
    "        dse = dse.select(range(len(dse) + test_range, len(dse)))\n",
    "\n",
    "\n",
    "if not individual_wer:\n",
    "    wers = trainer.evaluate(eval_dataset=dspe,)['eval_wer']\n",
    "    print(len(wers))\n",
    "    len(dse), len(wers)\n",
    "\n",
    "    werss = list(enumerate(wers))\n",
    "    # sort werss by wer\n",
    "    werss = sorted(werss, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare two different KenLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m4t_processor_with_lm import M4TProcessorWithLM\n",
    "processor_asr_lm = M4TProcessorWithLM.from_pretrained('/home/kd/Desktop/proj/apr/speech_pa/wav2vec2-bert-pa-lm_processor')\n",
    "processor_stories_lm = M4TProcessorWithLM.from_pretrained('/home/kd/Desktop/proj/apr/speech_pa/wav2vec2-bert-pa-lm_processor_stories_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "wers = {}\n",
    "wers_asr_lm = {}\n",
    "wers_stories_lm = {}\n",
    "pred = {}\n",
    "pred_asr_lm = {}\n",
    "pred_stories_lm = {}\n",
    "gt = {}\n",
    "from tqdm import tqdm\n",
    "for i in range(len(dspe)):\n",
    "    print(f'Processing {i}...')\n",
    "    input_features = torch.tensor(dspe[i][\"input_features\"]).to(\"cuda\").unsqueeze(0)\n",
    "    labels = torch.tensor(dspe[i][\"labels\"]).to(\"cuda\").unsqueeze(0) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_features, labels=labels)\n",
    "        # loss as double\n",
    "        loss = logits.loss.item()\n",
    "        losses[i] = loss\n",
    "\n",
    "        pred_ids = torch.argmax(logits.logits, dim=-1)[0]\n",
    "        pred[i] = processor.decode(pred_ids)\n",
    "\n",
    "        logits = logits.logits.cpu().detach().numpy()\n",
    "        pred_asr_lm[i] = processor_asr_lm.batch_decode(logits).text[0]\n",
    "        pred_stories_lm[i] = processor_stories_lm.batch_decode(logits).text[0]\n",
    "\n",
    "        wers[i] = wer_metric.compute(predictions=[pred[i]],references= [dse[i][\"normalized_text\"]])\n",
    "        wers_asr_lm[i] = wer_metric.compute(predictions=[pred_asr_lm[i]],references= [dse[i][\"normalized_text\"]])\n",
    "        wers_stories_lm[i] = wer_metric.compute(predictions=[pred_stories_lm[i]],references= [dse[i][\"normalized_text\"]])\n",
    "\n",
    "        # convert wer to percentage with 2 decimal points\n",
    "        wers[i] = round(wers[i] * 100, 2)\n",
    "        wers_asr_lm[i] = round(wers_asr_lm[i] * 100, 2)\n",
    "        wers_stories_lm[i] = round(wers_stories_lm[i] * 100, 2)\n",
    "\n",
    "        gt[i] = dse[i][\"normalized_text\"]\n",
    "\n",
    "        # update with su.pbprint()\n",
    "        pred[i] = su.pbprint(pred[i])\n",
    "        pred_asr_lm[i] = su.pbprint(pred_asr_lm[i])\n",
    "        pred_stories_lm[i] = su.pbprint(pred_stories_lm[i])\n",
    "        gt[i] = su.pbprint(gt[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_asr_lm</th>\n",
       "      <th>pred_stories_lm</th>\n",
       "      <th>wer_pred</th>\n",
       "      <th>wer_asr_lm</th>\n",
       "      <th>wer_stories_lm</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...</td>\n",
       "      <td>ਬਲਾਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਨਾ...</td>\n",
       "      <td>ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...</td>\n",
       "      <td>ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...</td>\n",
       "      <td>15.38</td>\n",
       "      <td>3.85</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.103631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਝ ਤਾਂ ਆਕਾਰ ਵਿੱਚ...</td>\n",
       "      <td>ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੇਵ ਹੈ ਜੋ ਉੰਜ ਦਾ ਘਾਰ ਵਿੱਚ ਛ...</td>\n",
       "      <td>ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਝ ਦਾ ਘਰ ਵਿੱਚ ਛੋ...</td>\n",
       "      <td>ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਜ ਦਾ ਘਰ ਵਿੱਚ ਛੋ...</td>\n",
       "      <td>12.20</td>\n",
       "      <td>4.88</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.211934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...</td>\n",
       "      <td>ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ ...</td>\n",
       "      <td>ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...</td>\n",
       "      <td>ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...</td>\n",
       "      <td>7.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.025127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ਅਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...</td>\n",
       "      <td>ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...</td>\n",
       "      <td>ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...</td>\n",
       "      <td>ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੋਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...</td>\n",
       "      <td>23.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.233114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾਗਾਸ...</td>\n",
       "      <td>ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾ ਗਾ...</td>\n",
       "      <td>ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾਗਾਸ...</td>\n",
       "      <td>ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੈਡਾਗਾਸ...</td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.087635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ground_truth  \\\n",
       "0  ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...   \n",
       "1  ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਝ ਤਾਂ ਆਕਾਰ ਵਿੱਚ...   \n",
       "2  ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...   \n",
       "3  ਅਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...   \n",
       "4  ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾਗਾਸ...   \n",
       "\n",
       "                                          prediction  \\\n",
       "0  ਬਲਾਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਨਾ...   \n",
       "1  ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੇਵ ਹੈ ਜੋ ਉੰਜ ਦਾ ਘਾਰ ਵਿੱਚ ਛ...   \n",
       "2  ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ ...   \n",
       "3  ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...   \n",
       "4  ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾ ਗਾ...   \n",
       "\n",
       "                                         pred_asr_lm  \\\n",
       "0  ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...   \n",
       "1  ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਝ ਦਾ ਘਰ ਵਿੱਚ ਛੋ...   \n",
       "2  ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...   \n",
       "3  ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੌਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...   \n",
       "4  ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੇਡਾਗਾਸ...   \n",
       "\n",
       "                                     pred_stories_lm  wer_pred  wer_asr_lm  \\\n",
       "0  ਬਲੌਗਿੰਗ ਇੱਕ ਅਜਿਹਾ ਸਾਧਨ ਹੈ ਜੋ ਸਹਿਯੋਗ ਦੀ ਪ੍ਰੇਰਣਾ...     15.38        3.85   \n",
       "1  ਯੂਰਪ ਇੱਕ ਅਜਿਹਾ ਮਹਾਂਦੀਪ ਹੈ ਜੋ ਉਂਜ ਦਾ ਘਰ ਵਿੱਚ ਛੋ...     12.20        4.88   \n",
       "2  ਸੱਭਿਅਤਾ ਸ਼ਬਦ ਲੈਟਿਨ ਸੱਭਿਅਤਾ ਤੋਂ ਆਇਆ ਹੈ ਜਿਸਦਾ ਅਰਥ...      7.14        0.00   \n",
       "3  ਆਸਟ੍ਰੇਲੀਆ ਦੇ ਮਿਚਲ ਗੋਰਲੀ ਨੇ ਪੁਰਸ਼ਾਂ ਦੇ ਸਟੈਂਡਿੰਗ ...     23.33        3.33   \n",
       "4  ਜਦੋਂ ਜੰਗਲੀ ਜਾਨਵਰਾਂ ਦੀ ਗੱਲ ਆਉਂਦੀ ਹੈ ਤਾਂ ਮੈਡਾਗਾਸ...      9.09        0.00   \n",
       "\n",
       "   wer_stories_lm      loss  \n",
       "0            7.69  0.103631  \n",
       "1            7.32  0.211934  \n",
       "2            0.00  0.025127  \n",
       "3           30.00  0.233114  \n",
       "4            4.55  0.087635  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'ground_truth': gt,\n",
    "    'prediction': pred,\n",
    "    'pred_asr_lm': pred_asr_lm,\n",
    "    'pred_stories_lm': pred_stories_lm,\n",
    "    'wer_pred': wers,\n",
    "    'wer_asr_lm': wers_asr_lm,\n",
    "    'wer_stories_lm': wers_stories_lm,\n",
    "    'loss': losses,\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(465, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by wer_pred\n",
    "dfx = df.sort_values(by='wer_pred', ascending=False)\n",
    "for i, row in dfx.iterrows():\n",
    "    print(f'GT:              {row[\"ground_truth\"]}')\n",
    "    print(f'PRED:            {row[\"prediction\"]}')\n",
    "    print(f'PRED ASR LM:     {row[\"pred_asr_lm\"]}')\n",
    "    print(f'PRED STORIES LM: {row[\"pred_stories_lm\"]}')\n",
    "    print(f'Loss: {row[\"loss\"]} WER: {row[\"wer_pred\"]} WER ASR LM: {row[\"wer_asr_lm\"]} WER STORIES LM: {row[\"wer_stories_lm\"]} ')\n",
    "    ipd.display(ipd.Audio(data=dse[i][\"audio\"][\"array\"], autoplay=False, rate=16000))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "# df.to_csv(f'fluers_benchmark_asr_various_decoding_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Form Speech Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ਦਲਜੀਤ ਕੰਨ ਖਲ ਕੇ ਮੇਰੀ ਗੱਲ ਸੁਨਲਾ ਦੋ ਮਿੰਟ ਲਿਉ ਕੇ ਤੂੰ ਟੂਰ ਤੇ ਗਿਆ ਜਾਂ ਇਹਨਾਂ ਭੰਗੜਾਂ ਪਾਕੇ ਬੰਦ ਨੂੰ ਭੱਖ ਦਾਂ ਲੱਗਦੀ ਆ ਰੋਟੀ ਖਾਣ ਨੂੰ ਜੀ ਕਰ ਦਾਈ ਆ ਵਰਾਈਸ ਸਾੳ ਮੈਨੂੰ ਹਾਇਰ ਕਰ ਦੋ ਟੀਮ ਤੇ ਯੂਨੀਅਰ ਰੋਟੀ ਮੇਕਰ ਆਵੀਅਸ ਲੀ ਮੈਂ ਦੋ ਮਿੰਟ ਤਾਂ ਬਣਾ ਦੂਗੀ ਦਗਰਦਗਰਦੈ ਖਿਲਾਦੂਗੀ ਆਪ ਬੁਰਕੀਆਂ ਕਰ ਕਰਕੇ ਖਿਲਾ ਦੂਗੀ ਤੇਨੂੰ ਖੇਲ ਲਕਿਂ ਮੈਂ ਤਾਂ ਵੇਲੀ ਆਂ ਮੈਨੂੰ ਤਾਂ ਕੋਈ ਕੰਮ ਹਿਣੀਗਾ ਮੈਂ ਤਾ ਾੱਜੇ ਲਬੀ ਪਈ ਆ ਇਕ ਵੱਜ ਗਿਆ ਸੋ ਲਈ ਆਮ ਜਿਸਿਗ ਇਸ ਇਸ ਗੁਰ ਬਿਸਨਿਸ ਪ੍ਰਾਪ ਸੀ ਆਲਸੋ ਆਿਮ ਵੈਰੀ ਪ੍ਰਿਟੀ ਆ ਦੇਖ ਲੋ ਸਾਹੁਣੀ ਤਾਂ ਮੈਂ ਬਹੁਤ ੀ ਆ ਬਸ ਐਸ ਵੇਲੇ ਸੂਤੀ ਉਠੀ ਊੰ ਆ ਸੋ ਪਲੀਜ਼ ਡੋਂਟ ਮਾਈਂਡ ਆਹ ਦੇਖ ਲੋ ਉਸ ਸ਼ਿਟ ਕਿੱਥੇ ਮਰ ਗਈ ਓਏ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pr = processor_asr_lm\n",
    "\n",
    "pipe = pipeline('automatic-speech-recognition', model=model, tokenizer=pr.tokenizer, feature_extractor=pr.feature_extractor, decoder=pr.decoder, return_timestamps='word', device='cuda:0')\n",
    "\n",
    "output = pipe(\"dil_full_mono.wav\", chunk_length_s=20, stride_length_s=(6, 6))\n",
    "su.pbprint(output['text'])\n",
    "# print(output['chunks'])\n",
    "\n",
    "def convert_to_hms(seconds: float) -> str:\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = round((seconds % 1) * 1000)\n",
    "    output = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Transcript as .srt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ਪਹਿਲਾਂ ਵੀ ਲੋਕ ਗੱਲਾਂ ਕਰਦੇ ਸਨ ਵੀ ਤੂੰ ਕੁਛ ਕਰ ਨੀ ਰਿਹਾ\n",
      "ਤੇ ਜਦੋਂ ਪਗੜੀ ਸੈਂਟਰ ਖੋਲ ਤਾਂ ਉਦੋਂ ਵੀ ਲੋਕੀ ਗੱਲਾਂ ਕਰਨ ਲੈ ਤੂੰ ਚੰਗਾ\n",
      "ਭਲਾ ਸਰਦਾਰਾਂ ਦਾ ਮੁੰਡਾ ਚੰਗਾ ਭਲਾ ਜੱਟਾਂ ਦਾ ਮੁੰਡਾ ਜ਼ਮੀਨ ਜਾਦਾ\n",
      "ਤੇਰੇ ਕੋਲ ਤੇ ਤੂੰ ਸੌ ਸੌ ਰੁਪਏ ਬੱਦਲ ਪੱਗ ਬੰਨੇਗਾ ਮੈਂ ਕਹਿੰਦੀ\n",
      "ਅੱਜ ਦੋ ਸੌ ਕੱਲ੍ਹ ਨੂੰ ਦੋ ਹਜ਼ਾਰ ਹੋਏਗਾ ਤੇ ਪਰ\n",
      "ਸੋਨੂੰ ਚਾਲੀ ਹਜ਼ਾਰ ਵੀ ਹੋਏਗਾ ਪਹਿਲਾਂ ਤੇ ਮੈਨੂੰ ਇਕ ਮਿੰਟ\n",
      "ਯਕੀਨੀ ਨੀ ਹੋਇਆ ਵੀ ਦਲਜੀਤ ਭਾਜੀ ਵਾਸਤੇ ਮੈਨੂੰ ਫੋਨ ਆਇਆ ਪੱਗ ਬੰਨਣ\n",
      "ਦੀ ਜਦੋਂ ਦਸ ਬਾਏ ਬਾਰਾਂ ਦੇ ਕਮਰੇ\n",
      "ਦੇ ਵਿੱਚ ਵੀਹ ਵੀਹ ਪੱਗਾਂ ਮੈਂ ਸੌ ਸੌ ਦੇ ਹਿਸਾਬ ਨਾਲ\n",
      "ਬੰਨਾਉਂਦਾ ਸਾਂ ਤੇ ਅੱਜ\n",
      "ਤੁਸੀਂ ਆ ਗਾਣਾ ਤੇ ਸੁਣਿਆ ਈ ਹੋਣਾ ਆ ਕੇ ਪੱਗਾਂ\n",
      "ਪੋਛਵੀਆਂ ਵਾਲੇ ਰਾਹੀਂ ਬਚ ਕੇ ਨੀ ਰੰਗਲੇ ਦੁਪੱਟੇ ਵਾਲੀਏ\n",
      "ਤੁਸੀਂ ਆਹ ਵੀ ਗਾਣਾ ਸੁਣਿਆ ਹੋਣਾ ਮੂਹਰੇ ਬੰਨ੍ਹ ਕੇ ਗਲਾਬੀ ਪੱਗ\n",
      "ਖੜ੍ਹਦਾ ਨੀ ਪੁੱਤ ਜੱਟਾਂ ਦਾ ਏ ਗੱਭਰੂ ਤੁਸੀਂ ਪੱਗਾਂ ਦੇ ਉੱਤੇ ਗਾਣੇ\n",
      "ਤਾਂ ਬਹੁਤ ਸੁਣੇ ਹੋਣਗੇ ਪਰ ਕਦੀ ਇਹ ਨਹੀਂ ਸੁਣਿਆ ਹੋਣਾ\n",
      "ਵੀ ਪੱਗ ਨੇ ਕਿਸੇ ਦੀ ਜ਼ਿੰਦਗੀ ਬਦਲ ਦਿੱਤੀ ਸਤਸ੍ਰੀਕਾਲ ਜੀ\n",
      "ਸਾਰਿਆਂ ਨੂੰ ਮੈਂ ਗੁਰਪ੍ਰਤਾਪ ਸਿੰਘ ਕੰਗ ਪਿੰਡ ਰਣਜੀਤ\n",
      "ਨਗਰ ਗੌਂਦਰ ਜ਼ਿਲ੍ਹਾ ਕਰਨਾਲ ਹਰਿਆਣਾ\n",
      "ਮੈਂ ਤਹਾਨੂੰ ਦੱਸਣਾ ਆ ਕਿਵੇਂ ਪੱਗ ਨੇ ਮੇਰੀ ਜ਼ਿੰਦਗੀ ਬਦਲ\n",
      "ਦਿੱਤੀ ਮੈਨੂੰ ਸ਼ੁਰੂ ਤੋਂ ਹੀ ਪੱਗ ਦਾ ਬੜਾ ਸ਼ੌਂਕ ਸੀ\n",
      "ਬੜਾ ਚਾਅ ਸੀ ਅਕਸਰ ਮੈਂ ਆਪਣੇ ਫਾਦਰ ਸਾਹਿਬ ਨੂੰ\n",
      "ਵੀ ਪੱਗ ਬੰਨ ਦਿਆਂ ਵੇਂਦਾ ਹੁੰਦਾ ਸਾਂ ਤੇ ਫਿਰ ਮੈਂ ਟਰਾਈ\n",
      "ਕਰਦਾ ਹੁੰਦਾ ਸਾਂ ਵੀ ਇਹ ਕਿਵੇਂ ਪੂਣੀ ਕਰਦੇ ਨੇ ਕਿਵੇਂ ਇਹ ਬੰਨਦੇ ਨੇ ਫਿਰ\n",
      "ਮੈਂ ਆਪਣੇ ਤੇ ਟਰਾਈ ਕਰਦਾ ਹੁੰਦਾ ਸਾਂ ਤੇ ਪੱਗ\n",
      "ਹੌਲੀ ਹੌਲੀ ਮੈਂ ਹਰ ਰੋਜ਼ ਪ੍ਰੈਕਟਿਸ ਕਰਨੀ ਮੇਰੇ ਯਾਰ ਦੋਸਤ ਮੇਰੇ ਭਰਾਵਾਂ\n",
      "ਨੇ ਕਹਿਣਾ ਕਰਨਾ ਆ ਖੇਡੀਏ ਮੈਂ ਉਹਨਾਂ ਨੂੰ ਕਹਿਣਾ ਕਰਨਾ\n",
      "ਨਹੀਂ ਮੈਂ ਖੇਡਣਾ ਨੀ ਮੈਂ ਸਿਰਫ ਪੱਗ ਦੀ ਪ੍ਰੈਕਟਿਸ ਕਰਨੀ ਕਿਉਂਕਿ\n",
      "ਮੈਨੂੰ ਪੱਗ ਦਾ ਬਹੁਤ ਜਿਆਦਾ ਚਾਹ ਸੀ ਜਦੋਂ\n",
      "ਪੱਗ ਬੰਨਦੇ ਬੰਨਦੇ ਇੱਕ ਦਿਨ ਐਵੇਂ ਦਾ ਆਇਆ ਪੱਗ ਬਹੁਤ ਸੋਹਣੀ ਵੱਧ\n",
      "ਗਈ ਮੈਨੂੰ ਖੁਦ ਨੂੰ ਵੀ ਯਕੀਨ ਨਾ ਹੋਇਆ ਵੀ ਇੰਨੀ ਸੋਹਣੀ ਪੱਗ ਵੱਧ ਗਈ ਮੇਰੇ\n",
      "ਤੋਂ ਤੇ ਮੈਂ ਫਾਦਰ ਸਾਬ ਨੂੰ ਕਿਹਾ ਪੱਗ ਬਹੁਤ ਸੋਹਣੀ ਆਈ ਏ ਮੈਨੂੰ ਤੁਸੀਂ\n",
      "ਇੱਕ ਫੋਟੋ ਖਿਚਵਾ ਕੇ ਲਿਆਓ ਸਟੂਡੀਓ ਚ ਤੇ ਮੇਰੇ ਫਾਦਰ\n",
      "ਸਾਹਿਬ ਮੈਨੂੰ ਲੈ ਕੇ ਗਏ ਨੀਂਗ ਸਟੂਡੀਓ ਚ ਉਥੋਂ ਫੋਟੋ ਕਰਵਾਈ\n",
      "ਜਾ ਉਹ ਛੋਟਾ ਕੰਮ ਸੀ ਜਾ ਨੀ ਸੰਗ ਜਾਣਾ ਜਾਂ ਪਿੰਡ ਜਾਣਾ ਸ਼ਹਿਰ\n",
      "ਜਾਣਾ ਰਿਸ਼ਤੇਦਾਰੀ ਜਾਣਾ ਮੈਂ ਹਰ ਰੋਜ਼ ਪੱਗ ਬੰਨਦਾ ਸਾਂ ਕਿਉਂਕਿ\n",
      "ਮੈਨੂੰ ਬੜਾ ਚਾਅ ਸੀ ਹਰ ਰੋਜ਼ ਪੱਗ ਬੰਨਣ ਦਾ ਮੈਨੂੰ ਸੀ ਮੈਂ ਸੋਹਣੀ\n",
      "ਪੱਗ ਬੰਨਾਂ ਜਿਵੇਂ ਜਿਵੇਂ ਪੱਗ ਬੰਨਦਾ ਗਿਆ ਰਿਸ਼ਤੇਦਾਰ\n",
      "ਸਾਰੇ ਮੈਨੂੰ ਸਲਾਹ ਦੇ ਗਏ ਯਾਰ ਦੋਸਤ ਭਰਾ ਸਾਰਿਆ ਨੇ ਬੜੀ ਸੋਹਣੀ\n",
      "ਪੱਗ ਤੂੰ ਬੰਨਾ ਪਰ ਪੱਗ\n",
      "ਦੇ ਨਾਲ ਨਾਲ ਜਿਵੇਂ ਸਟੱਡੀ ਮੇਰੀ ਚੱਲਦੀ ਰਹੀ ਪਲੱਸ\n",
      "ਟੂ ਪਾਸ ਕਰਦੀ ਮੈਂ ਚੰਡੀਗੜ੍ਹ ਆ ਗਿਆ ਤੇ ਚੰਡੀਗੜ੍ਹ\n",
      "ਮੈਂ ਆਪਣੀ ਸਟੱਡੀ ਵੀ ਨਾਲ ਨਾਲ ਜਾਰੀ ਰੱਖੀ\n",
      "ਤੇ ਨਾਲ ਨਾਲ ਮੈਨੂੰ ਕੁਛ ਇਹ ਸੀ ਮੈਂ ਇੱਕ\n",
      "ਆਪਣੇ ਪੈਰਾਂ ਤੇ ਖੜ੍ਹਾ ਕਿੰਨਾ ਕੁ ਦੇਰ ਮੈਂ ਕਰਦਿਆਂ ਤੂੰ ਖਰਚਾ\n",
      "ਮੰਗਵਾ ਕੇ ਆਪਣਾ ਫੀਸ ਸੰਪਾਂਗਾ ਮੈਂ ਕਦੋਂ ਥਾਂ\n",
      "ਟਰਾਈ ਕੀਤਾ ਜੋਬ ਕਰਨ ਦੀ ਪਰ ਮੈਨੂੰ ਕਿਤੇ ਵੀ ਜੋਬ ਨਹੀਂ ਮਿਲੀ ਮੈਨੂੰ\n",
      "ਕਿਸੇ ਦੱਸਿਆ ਵੀ ਕੋਲ ਸੈਂਟਰ ਤੁਸੀਂ ਜੋਬ ਕਰ ਸਕਦੇ ਓ\n",
      "ਤੇ ਪਲੱਸ ਟੂ ਬੇਸ ਦੇ ਉੱਤੇ ਮੈਂ ਕੋਲ ਸੈਂਟਰ ਜੋਬ\n",
      "ਕਰ ਲਈ ਜੁਆਇਨ ਕਰ ਲਈ ਕੋਲ ਸੈਂਟਰ ਜਦੋਂ ਜੋਬ ਕੀਤੀ ਮੇਰਾ ਪੈਸ਼ਨ\n",
      "ਨਹੀਂ ਸੀ ਕੋਲ ਸੈਂਟਰ ਮੈਂ ਮੇਰਾ ਕੋਈ ਸਿਰਫ ਇੱਕ ਪੈਸੇ ਦੀ ਨੀਡਸਿੱਖ\n",
      "ਸ਼ੁਰੂਆਤ ਸੀ ਵੀ ਹਾਂ ਮੈਂ ਕੁਛ ਆਪਣੇ ਲਈ ਕਰਨਾ ਜਦੋਂ\n",
      "ਇੱਕ ਮਹੀਨਾ ਉੱਥੇ ਜੋਬ ਕੀਤੀ ਤੇ ਇੱਕ ਮਹੀਨੇ ਬਾਅਦ ਮੈਨੂੰ ਪੰਜ ਹਜ਼ਾਰ\n",
      "ਸੈਲਰੀ ਮਿਲੀ ਜਦੋਂ ਉਹ ਸੈਲੀ ਮਿਲੀ ਮੈਨੂੰ\n",
      "ਬਹੁਤ ਜਿਆਦਾ ਖੁਸ਼ੀ ਹੋਈ ਤੇ ਨਾਲ ਪਰ ਮੈਂ\n",
      "ਉਹਨੂੰ ਕੰਟੀਨਿਊ ਨਹੀਂ ਹਾਲ ਕਰ ਸਕਦਾ ਕਿਉਂਕਿ ਮੈਂ ਆਪਣੀ ਜਿੰਦਗੀ ਨੂ ਉਥੇ ਰੁਕਿਆ\n",
      "ਨੀ ਸਾਰਿਆਂ ਦੇਣਾ ਚਾਹੁੰਦਾ ਵੀ ਹਰ ਰੋਜ ਜਾਓ ਤੇ ਫੋਨ ਸੁਣੋ ਅੱਠ ਅੱਠ ਦਸ ਦਸ\n",
      "ਘੰਟੇ ਫੋਨ ਤੇ ਗੱਲਾਂ ਕਰੋ ਤੇ ਜਾਓ ਪਰ\n",
      "ਹੋਰ ਮੈਨੂੰ ਕੋਈ ਚਾਰਾ ਨਹੀਂ ਸੀ ਲੱਭ ਰਿਹਾ ਮੈਂ ਦੋ\n",
      "ਚਾਰ ਮਹੀਨੇ ਜੋਬ ਕੀਤੀ ਮੈਂ ਮਨ ਚ ਸੋਚਿਆ ਉਹ ਨਾ ਪਿੰਡ ਹੀ ਚੱਲ ਜਾਣਾ\n",
      "ਵਾਪਸ ਆ ਪਰ ਮਨ ਦੇ ਵਿੱਚ ਕਿਤੇ ਇਹ ਵੀ ਖਿਆਲ\n",
      "ਹੁੰਦਾ ਸੀ ਜਦੋਂ ਪਿੰਡ ਜਾਗਾ ਤੇ ਪਿੰਡਾਂ ਲਈ ਕੀ ਕਹਿਣਗੇ ਜਿਵੇਂ ਅਕਸਰ\n",
      "ਕਰਦੇ ਨੇ ਗੱਲਾਂ ਵਿਲਾ ਹੁਣ ਵਕੀਲ ਬਣ ਗਿਆ ਤੋਂ ਕਿਉਂਕਿ\n",
      "ਬਣਿਆ ਕੁਛ ਨਹੀਂ ਸਾਂ ਤੇ ਪਿੰਡ ਜਾਂ ਫਿਰ ਮਨ ਚ ਸੋਚਨਾ ਥਾਂ\n",
      "ਨਹੀਂ ਚੰਡੀਗੜ੍ਹ ਰਹੀ ਨਾ ਫਿਰ ਮਨ ਸੋਚਣਾ ਨਹੀਂ ਪਿੰਡ ਜਾਂ ਫਿਰ ਖਿਆਲ\n",
      "ਹੁੰਦਾ ਸੀ ਕਿਤੇ ਪਿੰਡ ਜੋਂਗੀ ਨਾ ਰਹਿ ਜਾ ਆਖਰਕਾਰ ਮੈਂ ਮਨ ਬਣਾਈ\n",
      "ਲਿਆ ਮੈਂ ਕਿਹਾ ਨਹੀਂ ਪਿੰਡ ਚੱਲਦਿਆ ਕੁਛ ਨਵਾਂ ਕਰਦੇ ਆਂ\n",
      "ਕੁਛ ਹੋਰ ਸ਼ੁਰੂਆਤ ਕਰਨੇ ਆ ਜਦੋਂ ਪਿੰਡ ਗਿਆ\n",
      "ਦੇ ਪਿੰਡਾਂ ਲਿਆ ਨੇ ਉਹੀ ਆਪਣੇ ਸਾਰਿਆਂ ਨੇ ਕਿਹਾ ਵੀ ਲੈ\n",
      "ਤੇ ਨੂੰ ਪੜ੍ਹਨ ਭੇਜਿਆ ਸੀ ਤੂੰ ਕੀ ਕਰਕੇ ਆ ਆ\n",
      "ਮੈਂ ਉੱਥੇ ਜੋਬ ਕਰਨ ਦੀ ਟਰਾਈ ਕੀਤੀ ਜਾਂ ਕੁਛ ਬਿਜ਼ਨਸ\n",
      "ਕਰਨ ਦਾ ਟਰਾਈ ਕੀਤਾ ਪਰ ਕੋਈ ਮੈਨੂੰ ਅੱਛਾ ਨਹੀਂ ਸੀ\n",
      "ਲੱਗ ਰਿਹਾ ਕੁਛ ਵੀ ਮੇਰਾ ਸ਼ੁਰੂ ਨਹੀਂ ਸੀ ਹੋ ਰਿਹਾ ਫਿਰ ਮੈਂ\n",
      "ਇੱਕ ਦਿਨ ਸੋਚਿਆ ਵੀ ਮੈਨੂੰ ਸਾਰੇ ਪੱਗ ਲਈ ਬੜਾ\n",
      "ਸਲਾਹੁੰਦੇ ਨੇ ਵੀ ਤੂੰ ਪੱਗ ਬੜੀ ਵਧੀਆ ਬੰਨਾ ਅਕਸਰ ਮੈਨੂੰ\n",
      "ਕਈ ਵਾਰ ਜਦੋਂ ਪੱਗ ਬੰਨ ਹੀ ਸਕੂਲ ਟਾਈਮ ਚ ਪੱਗ\n",
      "ਬੰਨਦਾ ਉਦੋਂ ਨਾਲ ਦੇ ਯਾਰ ਦੋਸਤ ਵੀ ਕਹਿ ਦਿੰਦੇ ਸਨ ਬਿਲ\n",
      "ਆਪਣੇ ਸਿਰ ਤੇ ਦੋ ਤੂੰ ਪੱਗ ਬੰਨ ਲੀਨਾ ਬੜੀ ਸੁਖੀ ਗੱਲ ਏ ਪਤਾ ਉਦੋਂ\n",
      "ਲੱਗੇਗਾ ਜਦੋਂ ਕਿਸੇ ਦੂਜੇ ਦੇ ਸਿਰ ਤੇ ਪੱਗ ਬਨੇਗਾ ਜਦੋਂ ਵੀ ਆਪਾਂ ਰਿਸ਼ਤੇਦਾਰੀ\n",
      "ਚ ਸਕੂਲ ਚ ਜਾਂ ਕੋਲਜ ਚ ਗਏ ਤਾਂ ਮੈਨੂੰ ਜਿੰਨੇ ਵੀ ਕਿਆ\n",
      "ਪੱਗ ਬੰਨਣ ਲਈ ਤੇ ਮੈਂ ਉਹਨੂੰ ਕਿਹੜੇ ਮੱਥੇ ਲਿਆ ਮੈਂ ਕਿਹਾ ਹਾਂ\n",
      "ਮੈਂ ਬਣਨਾ ਪੱਗ ਕਿਉਂਕਿ ਮੈਨੂੰ ਇੱਕ ਚਾਅ ਸੀ ਮੈਨੂੰ ਇੱਕ ਸੀ ਵੀ ਮੈਂ ਆਪਣੇ\n",
      "ਸਿਰ ਤੇ ਵੀ ਪੱਗ ਬੰਨਾ ਤੇ ਦੂਜੇ ਸਿਰ ਤੇ ਵੀ ਮੈਂ ਸੋਹਣੀ ਪੱਗ ਬੰਨਾ\n",
      "ਫਿਰ ਇੱਕ ਦਿਨ ਮੈਨੂੰ ਮਨ ਚ ਖਿਆਲ ਆਇਆ ਆਪਣੇ ਆਪ ਨਾਲ\n",
      "ਮੈਂ ਸਲਾਹ ਕੀਤੀ ਵੀ ਜੇ ਆਪਾਂ ਪੱਗ\n",
      "ਵੀ ਵਧੀਆ ਬਣ ਲੀਨੇਆਂ ਸਾਰੇ ਕਹਿੰਦੇ ਨੇ ਤੂੰ ਪੱਗ ਵਧੀਆ\n",
      "ਬੰਨ ਲਈ ਨਾ ਤਾਂ ਕਿਉਂ ਨਾ ਫਿਰ ਮੈਂ ਇੱਕ ਪੱਗੜੀ\n",
      "ਸੈਂਟਰ ਖੋਲਦੇ ਆ ਨਾਲੇ ਨਾਲੇ ਪੱਗ ਵੀ ਸਿਖਾਵਾਂਗੇ\n",
      "ਨਾਲੇ ਥੋੜਾ ਜਿਹਾ ਇਨਕਮ ਵੀ ਆਉਣੀ ਸ਼ੁਰੂ ਆਏਗੀ ਫਿਰ ਮੈਂ ਉੱਥੇ\n",
      "ਕ ਚੁਬਾਰਾ ਲਿਆ ਛੋਟਾ ਜਿਹਾ ਉਹਦਾ ਨਾਂ ਰੱਖਿਆ ਕੰਗ ਪੱਗੜੀ ਸੈਂਟਰ\n",
      "ਤੇ ਉਹ ਸ਼ੁਰੂ ਕਰ ਦਿੱਤਾ ਪਰ ਉਹਨਾਂ ਦਿਨਾਂ ਦੇ ਵਿੱਚ ਹੀ ਮੈਨੂੰ\n",
      "ਪਹਿਲਾਂ ਵੀ ਲੋਕ ਗੱਲਾਂ ਕਰਦੇ ਸਨ ਵੀ ਤੂੰ ਕੁਛ ਕਰ ਨੀ ਰਿਹਾ\n",
      "ਤੇ ਜਦੋਂ ਪੱਗੜੀ ਸੈਂਟਰ ਖੋਲੇ ਤਾਂ ਉਦੋਂ ਵੀ ਲੋਕੀ ਗੱਲਾਂ ਕਰਨ ਲੈ ਤੂੰ ਚੰਗਾ\n",
      "ਭਲਾ ਸਰਦਾਰਾਂ ਦਾ ਮੁੰਡਾ ਚੰਗਾ ਭਲਾ ਜੱਟਾਂ ਦਾ ਮੁੰਡਾ ਜ਼ਮੀਨ ਜਾਦਾ\n",
      "ਤੇਰੇ ਕੋਲ ਤੇ ਤੂੰ ਸੌ ਸੌ ਰੁਪਏ ਬਦਲ ਪੱਗ ਬਨੇਗਾ ਮੈਂ ਕਿਸੇ\n",
      "ਦੀ ਗੱਲ ਦੀ ਕੋਈ ਪਰਵਾਹ ਨਹੀਂ ਕੀਤੀ ਤੇ ਜਦੋਂ ਮੈਂ ਉੱਥੇ ਆਪਣੇ ਸੈਂਟਰ\n",
      "ਤੇ ਬਹਿ ਗਿਆ ਪੱਗਾਂ ਬੰਨੀਆ ਹਰ ਕੋਈ\n",
      "ਦੂਲਾ ਵੀ ਆਇਆ ਪੱਗ ਬਣਾਣ ਕੋਈ ਆਮ ਬੰਦਾ ਪੱਗ ਬਣਾਉਣਾ ਆ ਸਾਰਿਆਂ\n",
      "ਨੂੰ ਪੱਗਾ ਬੰਨੀਆ ਤੇ ਜਦੋਂ ਮੈਂ ਇੱਕ ਮੰਥ ਤੋਂ ਬਾਅਦ\n",
      "ਸਾਰਾ ਸਾਫ ਕੀਤਾ ਬਿਜਲੀ ਪਾਣੀ\n",
      "ਰੈਂਟ ਸਾਰਾ ਖਰਚੇ ਕੱਢ ਕੇ ਮੈਂ ਜਦੋਂ ਸਾਫ ਕੀਤਾ ਤੇ ਮੈਨੂੰ\n",
      "ਜਿਹੜੀ ਆਮਦਨ ਸੀ ਉਹ ਦੋ ਸੌ ਰੁਪਈਆ ਨਿਕਲੀ ਵਿੱਚੋਂ ਤੇ\n",
      "ਮੈਂ ਉਹ ਦੋ ਸੌ ਨੂੰ ਇਹ ਨੀ ਹੌਸਲਾ ਨੀ ਹਾਰਿਆ\n",
      "ਵੀ ਮੈਨੂੰ ਦੋ ਸੌ ਏ ਤੇ ਮੈਂ ਇਹ ਕੰਮ ਛਡਦਾ ਨੀ ਮੈਨੂੰ ਆਪਣੇ ਆਪ ਤੇ ਭਰੋਸਾ\n",
      "ਸੀ ਮੈਂ ਕਹਿੰਦੀ ਅੱਜ ਦੋ ਸੌ ਏ ਕੱਲ੍ਹ ਨੂੰ ਦੋ ਹਜ਼ਾਰ\n",
      "ਹੋਏਗਾ ਤੇ ਪਰਸੂ ਚਾਲੀ ਹਜ਼ਾਰ ਵੀ ਹੋਏਗਾ ਮੈਨੂੰ\n",
      "ਆਪਣੇ ਆਪ ਤੇ ਭਰੋਸਾ ਸੀ ਮੈਂ ਕੁਛ ਕਰਾਂਗਾ ਵਧੀਆ\n",
      "ਠੀਕ ਮੇਰਾ ਪਗੜੀ ਸੈਂਟਰ ਉੱਥੇ ਵਧੀਆ ਚੱਲਣ ਲੱਗਿਆ ਪਰ ਜਿਹੜੀ ਇੱਕ\n",
      "ਮਨ ਦੇ ਵਿੱਚ ਇੱਕ ਕੁਛ ਕਰਨ ਦੀ ਇੱਛਾ ਤੇ\n",
      "ਕਿਸੇ ਮੁਕਾਮ ਤੇ ਪਹੁੰਚਣ ਦੀ ਇੱਛਾ ਸੀ ਉਹ ਹਾਲੀ\n",
      "ਮਨ ਦੇ ਵਿੱਚ ਬਾਕੀ ਸੀ ਉਹਨੀਂ ਦਿਨੀਂ ਮੈਨੂੰ ਮੇਰੇ ਫ੍ਰੈਂਡ ਦਾ ਫ਼ੋਨ\n",
      "ਆਇਆ ਤੇ ਫੋਨ ਤੇ ਕਹਿੰਦਾ ਵੀ ਦਲਜੀਤ ਦੋਸਾਂਝ\n",
      "ਭਾਜੀ ਦੇ ਪੱਗ ਬਣਨੀ ਸਵੇਰੇ ਅੰਮ੍ਰਿਤਸਰ ਆ ਜਾ ਪਹਿਲਾਂ ਤਾਂ\n",
      "ਮੈਨੂੰ ਇੱਕ ਮਿੰਟ ਯਕੀਨੀ ਨਹੀ ਹੋਇਆ ਫਿਰ ਮੈਂ ਉਹਨੂੰ ਕਿਹਾ ਮੈਂ ਕਿਹਾ\n",
      "ਤੂੰ ਮੇਰੇ ਨਾਲ ਮਜ਼ਾਕ ਨਾ ਕਰ ਭਾਜੀ ਤੇ ਖੁਦ ਆ ਹੀ ਨੀ ਸੋਹਣੀ ਪੱਗ ਬਣ\n",
      "ਲੈਂਦੇ ਨੇ ਕਉਂਕਿ ਮੈਂ ਉਹਨਾਂ ਦੀਆਂ ਹਰੀਆਂ ਫਿਲਮਾਂ ਸਾਰੇ ਗਾਣੇ ਉਹ\n",
      "ਉਹਨਾਂ ਦੇ ਵੇਖੇ ਨਹੀਂ ਸੋਹਣੀਆ ਉਹਨਾਂ ਬੱਗਾ ਦਸਤਾਰ ਆਪ ਉਹਨਾਂ ਸਜਾਈ\n",
      "ਸੀ ਮੈਂ ਕਿਹਾ ਨਹੀਂ ਤੂੰ ਝੂਠ ਬੋਲਣਾ ਹੁਣ ਜਦ ਜੇ ਤੈਨੂੰ ਹਰ ਕੋਈ\n",
      "ਕੰਮ ਏ ਮੇਰੇ ਨਾਲ ਤਾਂ ਮੈਂ ਤੇਰੇ ਕੋਲ ਆ ਜਾਂਦਾ ਜੇ ਤੂੰ ਪਰਸਨਲ\n",
      "ਮੈਨੂੰ ਕੋਈ ਹੋਰ ਕੰਮ ਦੱਸਣਾ ਇਸ ਬਹਾਨੇ ਮੈਨੂੰ ਤੂੰ ਅੰਮ੍ਰਿਤਸਰ ਨਾ ਸਕਦੇ\n",
      "ਉਹਦੀ ਗੱਲ ਤੇ ਮੈਨੂੰ ਯਕੀਨ ਤੇ ਨਹੀਂ ਆਇਆ ਪਰ ਫਿਰ ਵੀ ਮੈਂ\n",
      "ਉਹਦੇ ਕਹੇ ਤੇ ਮੈਂ ਸਵੇਰ ਦੇ ਟਰੇਨ ਫੜ ਕੇ ਅੰਮ੍ਰਿਤਸਰ ਆ ਗਿਆ\n",
      "ਰਸਤੇ ਦੇ ਵਿੱਚ ਮੈਨੂੰ ਬੜੇ ਮੇਰੇ ਮਨ ਦੇ ਵਿੱਚ ਵਲਵਲੇ ਆਏ\n",
      "ਕਦੀ ਸੋਚਾਂ ਨਹੀਂ ਇਹ ਝੂਠ ਬੋਲ ਰਿਹਾ ਕਦੀ ਸੋਚਿਆ\n",
      "ਨਹੀਂ ਇਹ ਠੀਕ ਈ ਕਹਿੰਦਾ ਆਉਣਾ ਵੀ ਸੱਚੀ ਹੁਣਾ\n",
      "ਕਦੀ ਸੋਚਿਆ ਨਹੀਂ ਮੈਂ ਇੱਕ ਹਰਿਆਣੇ ਤੋਂ ਮੈਨੂੰ ਕਿੱਥੋਂ ਫੋਨ ਆ ਉਨਾ\n",
      "ਨਹੀਂ ਇਹ ਝੂਠ ਬੋਲਦਾ ਹੋਰ ਕੋਈ ਗੱਲ ਨੀ ਘਰ ਦੇ ਕਰਦੇ ਅੰਮ੍ਰਿਤਸਰ\n",
      "ਮੈਂ ਜਦੋਂ ਉਤਰਿਆ ਮੈਨੂੰ ਹਯਾਤ ਹੋਟਲ ਉਹਨਾਂ ਕਿਆ ਪਹੁੰਚਣ ਲਈ ਤੇ ਮੈਂ ਰੂਮ\n",
      "ਨੰਬਰ ਪਤਾ ਕੀਤਾ ਤੇ ਪਹਿਲੀ ਵਾਰ ਮੈਂ ਫਾਈਵ ਸਟਾਰ ਹੋਟਲ ਚ ਐਂਟਰ\n",
      "ਕੀਤਾ ਮੈਨੂੰ ਕੁਝ ਸੁੱਧ ਨਹੀਂ ਸੀ ਰਿਹਾ ਪਰ ਮੈਂ ਪੁੱਛ\n",
      "ਕੇ ਫਿਰ ਵੀ ਰੂਮ ਨੰਬਰ ਗਿਆ ਉੱਪਰ ਰੂਮ ਦੇ\n",
      "ਬਾਕੀ ਭਾਜੀ ਉੱਥੇ ਬੈਠੇ ਸਨ ਡਰੈਕਟਰ ਸਾਹਿਬ ਬਹਿਠੇ ਸਨ\n",
      "ਕਸਟਮ ਡਰੈਕਟਰ ਸਾਬੇ ਸਨ ਮੈਨੂੰ ਇਕ ਵਾਰ\n",
      "ਇੰਜ ਹੋਇਆ ਜੇ ਆਪਾਂ ਹਨੇਰੇ ਦੇ ਕਮਰੇ ਵਿੱਚੋਂ ਇਕ ਰਸ਼ਨ ਵੱਲ ਜਾਨੇ ਆ\n",
      "ਤੇ ਦੋ ਮਿੰਟ ਲਈ ਆਪਾਂ ਨੂੰ ਕੁਛ ਨੀ ਦਿਸਦਾ ਸੀ\n",
      "ਮੇਰੇ ਨਾਲ ਵੀ ਐਵੇਂ ਹੋਇਆ ਮੈਨੂੰ ਇੱਕ ਲੱਗਾ\n",
      "ਵੀ ਯਕੀਨ ਨੀ ਹੋਇਆ ਵੀ ਮੈਂ ਇੱਥੇ ਖੜ੍ਹਾਂ\n",
      "ਪਰ ਫਿਰ ਵੀ ਮੈਂ ਆਪਣੇ ਅਪਣਾ ਗੱਲਾਂ ਕਰਾਂ ਵੀ ਜਿਵੇਂ ਨੂੰ ਇਹਦਾ ਮੌਕਾ\n",
      "ਇਦਾ ਚਾਂਸ ਮਿਲਿਆ ਕਿਉਂਕਿ ਸਾਰੀ ਉਮਰ ਲੰਘ\n",
      "ਜਾਂਦੀ ਏ ਮੌਕਾ ਨਹੀਂ ਮਿਲਦਾ ਮੌਕਾ ਬੜਾ\n",
      "ਘੱਟ ਤੇ ਵਾਰ ਵਾਰ ਮੌਕਾ ਬੜਾ ਘੱਟ ਮਿਲਦਾ\n",
      "ਤੇ ਮੈਂ ਆਪਣੇ ਆਪ ਨੂੰ ਕਹਿ ਰਿਹਾ ਸਾਂ ਵੀ ਤੇਨੂੰ ਅੱਜ ਇਹ ਮੌਕਾ\n",
      "ਮਿਲਿਆ ਤੂੰ ਆਪਣਾ ਦੋ ਸੌ ਪਰਸੈਂਟ ਇਹਦੇ ਚ ਤੂੰ ਬੈਸਟ\n",
      "ਕਰਨਾ ਜਦੋਂ ਮੈਂ ਉੱਥੇ ਪਹੁੰਚਿਆ ਤੇ\n",
      "ਭਾਜੀ ਨੇ ਮੈਨੂੰ ਕਿਹਾ ਵੀ ਮੈਂ ਉਹਨਾ ਨੂੰ ਪੁੱਛਿਆ ਕਿਵੇਂ ਦੀ ਪੱਗ\n",
      "ਬੰਨਣੀ ਉਹਨਾਂ ਦੱਸਿਆ ਵੀ ਥੋੜਾ ਜਾ ਆ ਸਟਾਈਲ ਮੈਂ ਪੂਣੀ ਕਰਕੇ\n",
      "ਤੇ ਪੱਗ ਬੰਨਣੀ ਸ਼ੁਰੂ ਕਰ ਦਿੱਤੀ ਜਦੋਂ ਪੱਗ ਬੰਨੀ ਭਾਜੀ\n",
      "ਕਹਿੰਦੇ ਵੀ ਪੱਗ ਠੀਕ ਐ ਪਰ ਥੋੜਾ ਜਿਹਾ ਜੇ ਆਪਾਂ ਐਵੇਂ ਚੇਂਜ ਕਰ ਲਈਏ\n",
      "ਮੈਂ ਕਿਹਾ ਠੀਕ ਐ ਜੀ ਮੈਂ ਪੱਗ ਦੁਬਾਰਾ ਸ਼ੁਰੂ ਕਰਤੀ ਬੰਨਣੀ\n",
      "ਉਸੇ ਟਾਈਮ ਉਹਨਾਂ ਕਿਹਾ ਤਾਂ ਪੱਗ ਵੇਖ ਕੇ ਕਹਿਣ ਲੱਗੇ ਠੀਕ ਐ ਜੀ\n",
      "ਵੀ ਆਪਾਂ ਕੱਲ੍ਹ ਤੋਂ ਸ਼ੂਟਿੰਗ ਸ਼ੁਰੂ ਕਰਨੇ ਆ ਮੈਨੂੰ ਯਕੀਨ ਨੀ ਸੀ ਹੋ ਰਿਹਾ\n",
      "ਖੁਦ ਤੇ ਵੀ ਇਹ ਕਿਵੇਂ ਹੋ ਗਿਆ ਇਹ ਸਾਰਾ ਕੁਝ\n",
      "ਪਰ ਮੈਨੂੰ ਉਦਣ ਲੱਗਾ ਜਦੋਂ ਮੈਂ ਪੱਗ ਬਣਨੀ ਸ਼ੁਰੂ\n",
      "ਕੀਤੀ ਸੀ ਮੇਰੇ ਦਿਮਾਗ ਦੇ ਵਿੱਚ ਇਹ ਚੱਲ ਰਿਹਾ ਸੀ ਵੀ ਜਿਹੜੀ ਮੈਂ\n",
      "ਇੰਨੇ ਸਾਲਾਂ ਤੋਂ ਮਿਹਨਤ ਕੀਤੀ ਏ ਉਹਦਾ ਜਲਟਾ ਜਿੰਨਿਕਲਨਾ\n",
      "ਸਭ ਤੋਂ ਪਹਿਲਾਂ ਮੈਂ ਕਾਰ ਫੋਨ ਕੀਤਾ ਘਰ ਦੇ ਕਰਦਿਆਂ ਨੂੰ ਮੈਂ ਖੁਸ਼ਖਬਰੀ\n",
      "ਦਿੱਤੀ ਵੀ ਮੈਂ ਸਿਲੈਕਟ ਆਂ ਤੇ ਕਰਦਿਆ ਨੂੰ ਨਾਲ ਵੀ\n",
      "ਕਹਿਤਾ ਵੀ ਹੁਣ ਬਾਕੀ ਦੇ ਕੱਪੜੇ ਮੈਨੂੰ ਭੱਜਵਾਦੀ ਕਿਉਂਕਿ\n",
      "ਮੈਂ ਕੱਪੜੇ ਜਿਆਦਾ ਲੈ ਕੇ ਨਹੀਂ ਸਾਂ ਆਇਆ ਪੰਜਾਹ ਦਿਨ ਅਗਲੇ ਦਿਨ ਤੋਂ\n",
      "ਹੀ ਪੰਜਾਹ ਦਿਨ ਲਗਾਤਾਰ ਸ਼ੂਟ ਚੱਲਿਆ ਉਹ ਫਿਲਮ ਸੀ ਪੰਜਾਬ\n",
      "ਨਾਈਟੀ ਏਟੀ ਫੋਰ ਉਸ ਤੋਂ ਬਾਅਦ ਜਿੰਨੀਆਂ\n",
      "ਵੀ ਭਾਜੀ ਦੀਆਂ ਫਿਲਮਾਂ ਆਈਆਂ ਜਿੰਨੇ ਵੀ ਗਾਣੇ ਆਏ\n",
      "ਨੇ ਸਾਰਿਆਂ ਦੇ ਵਿੱਚ ਪੱਗ ਬੰਨਣ ਦਾ ਮੌਕਾ ਮੈਨੂੰ ਮਿਲਿਆ\n",
      "ਜਿਨ੍ਹਾਂ ਵਿੱਚੋਂ ਜਿਨ੍ਹਾਂ ਵਿੱਚੋਂ ਫਿਲਮਾਂ ਨੇ ਸੂਰਮਾ\n",
      "ਪੰਜਾਬ ਉੱਨੀ ਸੌ ਚੁਰਾਸੀ ਛੜਾ ਉੜਤਾ\n",
      "ਪੰਜਾਬ ਮੈਂ ਭਾਜੀ ਦਾ ਬੜਾ ਸ਼ੁਕਰਗੁਜ਼ਾਰ\n",
      "ਹਾਂ ਕੀ ਉਹਨਾਂ ਮੇਰੇ ਕੰਮ ਨੂੰ ਮੇਰੀ ਮਿਹਨਤ ਨੂੰ\n",
      "ਉਹਨਾਂ ਸਮਝਿਆ ਤੇ ਮੈਨੂੰ ਇੰਨੇ ਜੋਗਾ ਸਮਝਿਆ\n",
      "ਮੈਂ ਭਾਜੀ ਦੇ ਨਾਲ ਘੱਟੋ ਘੱਟ ਅੱਠ ਤੋਂ ਦਸ ਕੰਟਰੀਆਂ\n",
      "ਚ ਟਰੈਵਲ ਹਾਂ ਮੈਂ ਜਦੋਂ\n",
      "ਦਸ ਬਾਏ ਬਾਰਾਂ ਦੇ ਕਮਰੇ ਦੇ ਵਿੱਚ ਵੀਹ ਵੀਹ ਪੱਗਾਂ\n",
      "ਮੈਂ ਸੌ ਸੌ ਦੇ ਹਿਸਾਬ ਨਾਲ ਬੰਨਦਾ ਆਉਂਦਾ ਸਾਂ ਤੇ\n",
      "ਅੱਜ ਮੈਂ ਕਰੋੜਾਂ ਦੇ ਸੈੱਟ ਦੇ ਉੱਤੇ\n",
      "ਪੱਗਾ ਬੰਨ੍ਹਣਾ ਇਹ ਮੇਰੇ ਲਈ ਬੜੀ ਮਾਣ ਵਾਲੀ ਗੱਲ ਏ\n",
      "ਇਹ ਮੇਰੀ ਮਿਹਨਤ ਰੰਗ ਲਿਆਈ ਏ ਕਿੱਥੇ ਮੈਂ ਇਕ ਕਰਨਾਲ\n",
      "ਜ਼ਿਲ੍ਹੇ ਦੇ ਛੋਟੇ ਜਿਹੇ ਪਿੰਡ ਤੋਂ ਉੱਠ ਕੇ ਕੁਝ ਸੁਪਨੇ ਲੈ ਕੇ ਅੱਗੇ ਵਧਿਆ\n",
      "ਸਾਂ ਅੱਜ ਮੈਂ ਵਿਦੇਸ਼ਾਂ ਦੇ ਵਿੱਚ ਅਨੇਕਾਂ ਕੰਟਰੀਆਂ\n",
      "ਦੇ ਵਿੱਚ ਜਾ ਕੇ ਮੈਂ ਆਪਣੇ ਸਪਨੇ ਪੂਰੇ ਕਰਨਾ ਉੱਥੇ ਮੈਨੂੰ\n",
      "ਵਾਚਣ ਦਾ ਮੌਕਾ ਮਿਲਦਾ ਤੁਸੀਂ ਵੀ ਹਰ ਸਪਨਾ ਪੂਰਾ ਕਰ\n",
      "ਸਕਦੇ ਓ ਜੇ ਤੁਸੀਂ ਸੱਚੀ ਸਾਫ਼ ਨੀਅਤ\n",
      "ਤੇ ਆਪਣੀ ਸ਼ਿੱਦਤ ਮਿਹਨਤ ਨਾਲ ਤੁਸੀਂ\n",
      "ਜੇ ਆਪਣਾ ਕੁਝ ਸੋਚ ਕੇ ਅੱਗੇ ਚੱਲ ਰਹੇ ਓ ਤੇ ਕੁਦਰਤ\n",
      "ਉਹ ਚੀਜ਼ ਤੁਹਾਨੂੰ ਜ਼ਰੂਰ ਦੇਵੇਗੀ ਤੁਸੀਂ ਇਹ ਸੋਚ\n",
      "ਕੇ ਨਿਰਾਸ਼ ਨਹੀਂ ਹੋਣਾ ਕਿ ਤੁਹਾਡੇ ਕੋਲ ਪੈਸੇ ਨਹੀਂ\n",
      "ਐ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਸਾਧਨ ਨਹੀਂ ਆ ਤੁਹਾਡੀ ਕੋਈ ਹੈਲਪ ਨਹੀਂ ਕਰ ਰਿਹਾ\n",
      "ਜੇ ਤੁਹਾਡਾ ਸੁਪਨਾ ਤੁਹਾਡੇ ਦਿਮਾਗ ਤੇ ਤੁਹਾਡੇ ਦਿਲ ਦੇ ਵਿੱਚ\n",
      "ਹੈ ਤੁਸੀਂ ਸੱਚੀ ਮਿਹਨਤ ਤੇ ਸੱਚੀ ਨੀਅਤ ਨਾਲ ਉਹ\n",
      "ਕੰਮ ਕਰ ਰਹੇ ਉਹ ਸੋਚ ਨੂੰ ਅੱਗੇ ਲੈ ਕੇ ਜਾ ਰਹੇ ਓ ਤੇ\n",
      "ਉਹ ਸਪਨਾ ਤੁਹਾਡੇ ਤੋਂ ਕੋਈ ਨਹੀਂ ਖੋਜ ਸਕਦਾ\n",
      "ਜੇ ਸਪਨੇ ਤੁਹਾਡੇ ਨੇ ਤੇ ਕੋਸ਼ਿਸ਼\n",
      "ਵੀ ਤੁਹਾਨੂੰ ਕਰਨੀ ਪੈਣੀ ਐ ਸੱਸਰੀਕਾਲ\n",
      "ਜੀ ਧੰਨਵਾਦ\n",
      "ਜੇ ਤੁਹਾਨੂੰ ਮੇਰੀ ਕਹਾਣੀ ਤੋਂ ਕੁੱਝ ਸਿੱਖਣ ਨੂੰ ਮਿਲਿਆ ਤੇ ਕਮੈਂਟ\n",
      "ਕਰਕੇ ਜਰੂਰ ਦੱਸਣਾ ਇਦਾਂ ਦੀਆਂ ਹੋਰ ਕਹਾਣੀਆਂ\n",
      "ਸੁਣਨ ਲਈ ਜੋਸ਼ ਟੋਕ ਦੇ ਚੈਨਲ ਨੂੰ ਸਬਸਕ੍ਰਾਈਬ\n",
      "ਲਾਈਕ ਸ਼ੇਅਰ ਜ਼ਰੂਰ ਕਰੋ\n"
     ]
    }
   ],
   "source": [
    "def merge_chunks_into_sentences(chunks):\n",
    "    sentences = []\n",
    "    current_sentence = {\"text\": \"\", \"start_time\": None, \"end_time\": None}\n",
    "    for chunk in chunks:\n",
    "        if current_sentence[\"start_time\"] is None:\n",
    "            current_sentence[\"start_time\"] = chunk[\"timestamp\"][0]\n",
    "\n",
    "        current_sentence[\"text\"] += \" \" + chunk[\"text\"].strip()\n",
    "\n",
    "        # Update end time for the current sentence\n",
    "        current_sentence[\"end_time\"] = chunk[\"timestamp\"][1]\n",
    "\n",
    "        # Check if the next chunk will break the sentence\n",
    "        if chunks.index(chunk) < len(chunks) - 1:\n",
    "            next_chunk = chunks[chunks.index(chunk) + 1]\n",
    "            if next_chunk[\"timestamp\"][0] - current_sentence[\"start_time\"] > 3.0:\n",
    "                # If the duration between the next chunk and the start of the sentence is greater than 1 second\n",
    "                # then consider it as a new sentence\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = {\"text\": \"\", \"start_time\": None, \"end_time\": None}\n",
    "    # Add the last sentence\n",
    "    sentences.append(current_sentence)\n",
    "    return sentences\n",
    "\n",
    "def convert_to_srt(sentences):\n",
    "    srt_content = \"\"\n",
    "    for i, sentence in enumerate(sentences, start=1):\n",
    "        start = convert_to_hms(sentence[\"start_time\"])\n",
    "        end = convert_to_hms(sentence[\"end_time\"])\n",
    "        text = sentence[\"text\"].strip()\n",
    "        text = su.pbprint(text)\n",
    "        srt_content += f\"{i}\\n{start} --> {end}\\n{text}\\n\\n\"\n",
    "    return srt_content\n",
    "\n",
    "# Assuming 'output' is the dictionary containing chunks\n",
    "chunks = output['chunks']\n",
    "sentences = merge_chunks_into_sentences(chunks)\n",
    "\n",
    "with open(\"file_asr_30sec.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(convert_to_srt(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect examples after soring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(index):\n",
    "    print(f'Index: {index}, WER: {wers[index]}')\n",
    "    # print(f'Normalized Text: {dse[index][\"normalized_text\"]}')\n",
    "    input_features = torch.tensor(dspe[index][\"input_features\"]).to(\"cuda\").unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_features).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "    su.pbprint(f'Prediction: {processor.decode(pred_ids)}')\n",
    "    su.pbprint(f'Ground Truth: {processor.decode(dspe[index][\"labels\"]).lower()}')\n",
    "    ipd.display(ipd.Audio(data=dse[index][\"audio\"][\"array\"], autoplay=False, rate=16000))\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i, wer in werss:\n",
    "    if count < 100 and count > 0:\n",
    "        # print( wer)\n",
    "        show_example(i)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
