{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import storage\n",
    "from typing import List\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import creds \n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "BUCKET_NAME = creds.gcp_bucket_name\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "client = SpeechClient(\n",
    "    client_options=ClientOptions(\n",
    "        api_endpoint=\"us-central1-speech.googleapis.com\",\n",
    "    )\n",
    ")\n",
    "config = cloud_speech.RecognitionConfig(\n",
    "    auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
    "    language_codes=[\"pa-Guru-IN\"],\n",
    "    model=\"chirp_2\",\n",
    "    features=cloud_speech.RecognitionFeatures(\n",
    "        enable_automatic_punctuation=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "def get_all_transcript_files(id, main_dir_prefix = creds.jt):\n",
    "    blobs = bucket.list_blobs(prefix=f'{main_dir_prefix}/transcriptions/{id}/')\n",
    "    return blobs\n",
    "\n",
    "def get_all_audio_split_files_uris(id, main_dir_prefix = creds.jt):\n",
    "    blobs = bucket.list_blobs(prefix=f'{main_dir_prefix}/audio_wavs/{id}/')\n",
    "    uris = []\n",
    "    for blob in blobs:\n",
    "        uri = f'gs://{BUCKET_NAME}/{blob.name}'\n",
    "        uris.append(uri)\n",
    "    return uris\n",
    "\n",
    "def send_batch_requests(uris_batch, id, main_dir_prefix = creds.jt):\n",
    "    files = [cloud_speech.BatchRecognizeFileMetadata(uri=uri) for uri in uris_batch]\n",
    "\n",
    "    request = cloud_speech.BatchRecognizeRequest(\n",
    "        recognizer=f\"projects/{creds.gcp_project_id}/locations/us-central1/recognizers/_\",\n",
    "        config=config,\n",
    "        files=files,\n",
    "        recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
    "            gcs_output_config=cloud_speech.GcsOutputConfig(\n",
    "                uri=f\"gs://{BUCKET_NAME}/{main_dir_prefix}/transcriptions/{id}\",\n",
    "            ),\n",
    "        ),\n",
    "        # processing_strategy=cloud_speech.BatchRecognizeRequest.ProcessingStrategy.DYNAMIC_BATCHING,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_recognize(request=request)\n",
    "    response = operation.result()\n",
    "    return response\n",
    "\n",
    "def get_all_ids():\n",
    "    d = '/mnt/sea/jt_wavs'\n",
    "    files = os.listdir(d)\n",
    "    files = [f for f in files if f.endswith('.wav')]\n",
    "    ids = [f.split(' - ')[1] for f in files]\n",
    "    return ids\n",
    "\n",
    "def transcribe_all():\n",
    "    ids = get_all_ids()\n",
    "\n",
    "    for id in ids:\n",
    "        print(f'Processing id: {id}')\n",
    "        uris = get_all_audio_split_files_uris(id, creds.jt)\n",
    "        transcript_blobs = get_all_transcript_files(id, creds.jt)\n",
    "        transcript_uris = []\n",
    "        for blob in transcript_blobs:\n",
    "            uri = f'gs://{BUCKET_NAME}/{blob.name}'\n",
    "            transcript_uris.append(uri)\n",
    "        uris = remove_uris_where_transcript_already_exist(uris, transcript_uris)\n",
    "        uris_batches = [uris[i:i + 15] for i in range(0, len(uris), 15)]\n",
    "        total_batches = len(uris_batches)\n",
    "        with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n",
    "            with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "                futures = [executor.submit(send_batch_requests, uris_batch, id) for uris_batch in uris_batches]\n",
    "                for future in as_completed(futures):\n",
    "                    pbar.update(1)\n",
    "                    time.sleep(1)\n",
    "\n",
    "def transcribe_specific_uris(start_sr_list, id):\n",
    "    uris = get_all_audio_split_files_uris(id, creds.jt)\n",
    "    uris_start_sr = [uri.split('__')[2].split('-')[0] for uri in uris]\n",
    "    selected_uris = []\n",
    "    for start_sr in start_sr_list:\n",
    "        selected_uris.append(uris[uris_start_sr.index(start_sr)])\n",
    "\n",
    "    uris_batches = [selected_uris[i:i + 15] for i in range(0, len(selected_uris), 15)]\n",
    "    total_batches = len(uris_batches)\n",
    "    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "            futures = [executor.submit(send_batch_requests, uris_batch, 'test1', 'testing') for uris_batch in uris_batches]\n",
    "            for future in as_completed(futures):\n",
    "                pbar.update(1)\n",
    "\n",
    "def transcribe_uri(uri, id):\n",
    "    files = [cloud_speech.BatchRecognizeFileMetadata(uri=uri)]\n",
    "\n",
    "    request = cloud_speech.BatchRecognizeRequest(\n",
    "        recognizer=f\"projects/{creds.gcp_project_id}/locations/us-central1/recognizers/_\",\n",
    "        config=config,\n",
    "        files=files,\n",
    "        recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
    "            gcs_output_config=cloud_speech.GcsOutputConfig(\n",
    "                uri=f\"gs://{BUCKET_NAME}/manual/transcriptions/{id}\",\n",
    "            ),\n",
    "        ),\n",
    "        processing_strategy=cloud_speech.BatchRecognizeRequest.ProcessingStrategy.DYNAMIC_BATCHING,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_recognize(request=request)\n",
    "    response = operation.result(timeout=1200)\n",
    "    return response\n",
    "\n",
    "def save_transcript(save_dir, id, main_dir):\n",
    "    blobs = get_all_transcript_files(id, main_dir)\n",
    "    results = []\n",
    "    for blob in tqdm(blobs):\n",
    "        result_bytes = bucket.blob(blob.name).download_as_bytes()\n",
    "        result = cloud_speech.BatchRecognizeResults.from_json(result_bytes, ignore_unknown_fields=True)\n",
    "        transcript = result.results[0].alternatives[0].transcript\n",
    "        metadata = blob.name.split('__')\n",
    "        id = metadata[0].split('/')[-1]\n",
    "        chunk_index = metadata[1]\n",
    "        timedata = metadata[2].split('_')[0]\n",
    "        start_sr = timedata.split('-')[0]\n",
    "        end_sr = timedata.split('-')[1]\n",
    "        sample_rate = metadata[3].split('_')[0]\n",
    "        results.append((id, chunk_index, start_sr, end_sr, sample_rate, transcript))\n",
    "\n",
    "    # cast chunk_index, start_sr, end_sr, sample_rate to int\n",
    "    results = [(id, int(chunk_index), int(start_sr), int(end_sr), int(sample_rate), transcript) for id, chunk_index, start_sr, end_sr, sample_rate, transcript in results]\n",
    "    print(f'Len of transcripts of {id}: {len(results)}')\n",
    "    df = pd.DataFrame(results, columns=['id', 'chunk_index', 'start_sr', 'end_sr', 'sample_rate', 'transcript'])\n",
    "    df['chunk_index'] = df['chunk_index'].astype(int)\n",
    "    df['start_sr'] = df['start_sr'].astype(int)\n",
    "    df['end_sr'] = df['end_sr'].astype(int)\n",
    "    df['sample_rate'] = df['sample_rate'].astype(int)\n",
    "    df = df.sort_values(by=['chunk_index'])\n",
    "    df.to_csv(f'{save_dir}/{id}.csv', index=False)\n",
    "\n",
    "def save_all_transcripts():\n",
    "    ids = get_all_ids()\n",
    "    ids = ['21m4IDjf-dc']\n",
    "    for id in ids:\n",
    "        save_transcript('/mnt/sea/jt_transcripts', id, creds.jt)\n",
    "\n",
    "def remove_uris_where_transcript_already_exist(audio_uris, transcript_uris):\n",
    "    # Extract the base identifier for each audio file (assuming the identifier is the file name without extension)\n",
    "    audio_ids = {uri.split('/')[-1].split('.')[0] for uri in audio_uris}\n",
    "    transcript_ids = ['_'.join(uri.split('_')[:-2]).split('/')[-1] for uri in transcript_uris]\n",
    "\n",
    "    result = []\n",
    "    for audio_id, audio_uri in zip(audio_ids, audio_uris):\n",
    "        if audio_id not in transcript_ids:\n",
    "            result.append(audio_uri)\n",
    "\n",
    "    if len(result) == 0:\n",
    "        print(\"All audio files have been transcribed.\")\n",
    "    elif len(result) != len(audio_uris):\n",
    "        print(f\"Removed {len(audio_uris) - len(result)} audio files where transcript already exists.\")\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "d = '/mnt/sea/yt_splits/singh_brar'\n",
    "td = '/mnt/sea/yt_transcripts/singh_brar'\n",
    "\n",
    "if not os.path.exists(td):\n",
    "    os.makedirs(td)\n",
    "\n",
    "def transcribe_gcs_v2(\n",
    "    audio_file: str,\n",
    ") -> cloud_speech.RecognizeResponse:\n",
    "    \n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    request = cloud_speech.RecognizeRequest(\n",
    "        recognizer=f\"projects/{creds.gcp_project_id}/locations/us-central1/recognizers/_\",\n",
    "        config=config,\n",
    "        content=content,\n",
    "    )\n",
    "\n",
    "    response = client.recognize(request=request)\n",
    "    return response\n",
    "\n",
    "def process_audio_file(audio_file):\n",
    "    return transcribe_gcs_v2(audio_file)\n",
    "\n",
    "\n",
    "def process_audio_files(audio_files, id):\n",
    "    transcripts = []\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "            # Submit tasks for each URI\n",
    "            future_to_uri = {executor.submit(process_audio_file, audio_file): audio_file for audio_file in audio_files}\n",
    "            with tqdm(total=len(audio_files)) as pbar:\n",
    "\n",
    "                for future in concurrent.futures.as_completed(future_to_uri):\n",
    "                    uri = future_to_uri[future]\n",
    "                    # Retrieve the result of each task\n",
    "                    response = future.result()\n",
    "                    transcript = response.results[0].alternatives[0].transcript\n",
    "                    transcripts.append(transcript)\n",
    "\n",
    "                    name = uri.split('/')[-1]\n",
    "                    metadata = name.split('__')\n",
    "                    if len(name.split('__')) == 5:\n",
    "                        id = metadata[0].split('/')[-1] + '__' + metadata[1]\n",
    "                    elif len(name.split('__')) == 4:\n",
    "                        id = metadata[0].split('/')[-1]\n",
    "                    else:\n",
    "                        raise Exception('Invalid name - multiple __ in it')\n",
    "                    chunk_index = metadata[-3]\n",
    "                    timedata = metadata[-2].split('_')[0]\n",
    "                    start_sr = timedata.split('-')[0]\n",
    "                    end_sr = timedata.split('-')[1]\n",
    "                    sample_rate = metadata[-1].split('_')[0].split('.')[0]\n",
    "                    results.append((id, chunk_index, start_sr, end_sr, sample_rate, transcript))   \n",
    "                    \n",
    "                    pbar.update(1) \n",
    "\n",
    "        print(len(transcripts))\n",
    "\n",
    "        df = pd.DataFrame(results, columns=['id', 'chunk_index', 'start_sr', 'end_sr', 'sample_rate', 'transcript'])\n",
    "        df['chunk_index'] = df['chunk_index'].astype(int)\n",
    "        df['start_sr'] = df['start_sr'].astype(int)\n",
    "        df['end_sr'] = df['end_sr'].astype(int)\n",
    "        df['sample_rate'] = df['sample_rate'].astype(int)\n",
    "        df = df.sort_values(by=['chunk_index'])\n",
    "        df.to_csv(f'{td}/{id}.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error occured for id: {id}')\n",
    "        print('Exception occured: ', e)\n",
    "\n",
    "dirs = os.listdir(d)\n",
    "t_files = os.listdir(td)\n",
    "\n",
    "count = 0\n",
    "print(f'Processing {len(dirs)} directories')\n",
    "for dir in dirs:\n",
    "    count += 1\n",
    "    if f'{dir}.csv' in t_files:\n",
    "        continue\n",
    "    print(f'Processing {dir} ... {count}')\n",
    "    files = os.listdir(f'{d}/{dir}')\n",
    "    files = [f'{d}/{dir}/{f}' for f in files]\n",
    "    process_audio_files(files, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JT DATA SAVE\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "jt_transcripts = '/mnt/sea/jt_transcripts'\n",
    "jt_splits = '/mnt/sea/jt_splits'\n",
    "\n",
    "transcript_files = os.listdir(jt_transcripts)\n",
    "dirs = [x[:-4] for x in transcript_files]\n",
    "\n",
    "rows = []\n",
    "df = pd.DataFrame()\n",
    "for d in dirs:\n",
    "    df_transcript = pd.read_csv(os.path.join(jt_transcripts, d + '.csv'))\n",
    "    df_transcript['audio'] = [f'{jt_splits}/{d}/{d}__{str(i)}__{str(row[\"start_sr\"])}-{str(row[\"end_sr\"])}__{str(row[\"sample_rate\"])}.wav' for i, row in df_transcript.iterrows()]\n",
    "    df_transcript = df_transcript.rename(columns={'transcript': 'text'})\n",
    "    df_transcript['duration'] = (df_transcript['end_sr'] - df_transcript['start_sr']) / df_transcript['sample_rate']\n",
    "    df = pd.concat([df, df_transcript])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149.81596521049855, 1.4444444444444444)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['duration']) / 3600, 13000/(150 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio as show_audio\n",
    "show_audio(df.iloc[0]['audio'], autoplay=True, rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "ds = ds.cast_column('audio', Audio(sampling_rate = 16000))\n",
    "print(ds)\n",
    "ds = DatasetDict({'train': ds})\n",
    "ds.save_to_disk('/mnt/sea/speech/punjabi_asr_datasets/jt_dataset')\n",
    "ds.push_to_hub('codingninja/jt_dataset', token=creds.hf_c_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT DATASET SAVE\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "transcripts = '/mnt/sea/yt_transcripts'\n",
    "splits = '/mnt/sea/yt_splits'\n",
    "\n",
    "source_dirs = os.listdir(transcripts)\n",
    "\n",
    "rows = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for source_dir in source_dirs:\n",
    "    source_path = os.path.join(transcripts, source_dir)\n",
    "    transcript_files = os.listdir(source_path)\n",
    "\n",
    "    for transcript_file in transcript_files:\n",
    "        basename = transcript_file[:-4]\n",
    "        df_transcript = pd.read_csv(os.path.join(source_path, transcript_file))\n",
    "        df_transcript['audio'] = [\n",
    "            f'{splits}/{source_dir}/{basename}/{basename}__{str(i)}__{str(row[\"start_sr\"])}-{str(row[\"end_sr\"])}__{str(row[\"sample_rate\"])}.wav'\n",
    "            for i, row in df_transcript.iterrows()\n",
    "        ]\n",
    "        df_transcript = df_transcript.rename(columns={'transcript': 'text'})\n",
    "        df_transcript['duration'] = (df_transcript['end_sr'] - df_transcript['start_sr']) / df_transcript['sample_rate']\n",
    "        df_transcript['source'] = source_dir  \n",
    "        df = pd.concat([df, df_transcript])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['duration']) / 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio as show_audio\n",
    "show_audio(df.iloc[0]['audio'], autoplay=True, rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sample_rate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "ds = ds.cast_column('audio', Audio(sampling_rate = 48000))\n",
    "print(ds)\n",
    "ds = DatasetDict({'train': ds})\n",
    "ds.save_to_disk('/mnt/pi/datasets/speech/yt_dataset')\n",
    "ds.push_to_hub(creds.yt_ds_hub, token=creds.hf_c_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe_all()\n",
    "\n",
    "# save_all_transcripts()\n",
    "\n",
    "\n",
    "# start_sr_test_list = ['409', '1513310', '2055563', '6532832', '39980477', '40872491', '44207098', '44546211', '46477679', '53141979', '54744982', '55656655', '65524505']\n",
    "\n",
    "# transcribe_specific_uris(start_sr_test_list, '21m4IDjf-dc')\n",
    "# save_transcript('/mnt/sea/jt_transcripts', 'test1', 'testing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
