{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
      "metadata": {
        "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
      },
      "source": [
        "# Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c32931142ef4a8f8bb0cd2cd7547056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset from disk:   0%|          | 0/167 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df66784f5a8d427aa8f6201a7c63d108",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset from disk:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "        num_rows: 113812\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "        num_rows: 16081\n",
            "    })\n",
            "})\n",
            "Dataset({\n",
            "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "    num_rows: 113812\n",
            "})\n",
            "Dataset({\n",
            "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "    num_rows: 16081\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(469.8450874682697, 38.31865094797178)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets, IterableDataset, load_from_disk\n",
        "\n",
        "ds = load_from_disk(\"/home/kd/Desktop/ds/Punjabi_ASR\")\n",
        "print(ds)\n",
        "# train + valid as train_ds\n",
        "train_ds = ds['train']\n",
        "valid_ds = ds[\"test\"]\n",
        "\n",
        "print(train_ds)\n",
        "print(valid_ds)\n",
        "sum(train_ds['duration']) / 3600, sum(valid_ds['duration']) / 3600\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff039fe2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa337338",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f1efa4a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "    num_rows: 113731\n",
            "})\n",
            "Dataset({\n",
            "    features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
            "    num_rows: 16080\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def remove_samples(data):\n",
        "    durations = data['duration']\n",
        "    texts = data['normalized_text']\n",
        "\n",
        "    indexes = [i for i in range(len(data)) if durations[i] < 0.150]\n",
        "\n",
        "    # # SPECIAL CONDITION FOR TESTING \n",
        "    # indexes = [i for i in range(len(data)) if (durations[i] < 0.150 or durations[i] > 10)]\n",
        "\n",
        "\n",
        "    indexes2 = [i for i in range(len(data)) if len(texts[i]) == 0]\n",
        "    \n",
        "    indexes_to_remove = list(set(indexes + indexes2))\n",
        "    x = data.select(indexes_to_remove)['normalized_text']\n",
        "    remaining_index = [i for i in range(len(data)) if i not in indexes_to_remove]\n",
        "    return data.select(remaining_index)\n",
        "\n",
        "\n",
        "empty_audio_index = [15081]\n",
        "valid_ds = valid_ds.select([i for i in range(len(valid_ds)) if i not in empty_audio_index])\n",
        "\n",
        "train_empty_audio_indexes_with_inf_loss = [751, 1118, 10355, 11512, 15486, 15637, 17497, 17947, 19759, 20274, 21598, 22600, 24869, 25557, 26849, 29608, 32231, 33120, 34988, 35147, 35733, 40806, 42817, 44421, 45757, 46451, 46600, 49925, 51102, 51557, 55828, 61721, 62382, 62604, 63597, 64926, 65199, 65784, 66141, 66226, 70457, 71123, 72796, 73234, 73990, 74982, 75546, 76065, 79491, 82027, 88235, 88962, 90183, 90332, 90418, 92732, 94826, 95229, 95664, 95724, 95819, 98712, 99752, 100006, 100548, 101055, 101866, 102255, 104914, 109203, 109814, 111464, 112353, 112905, 113190]\n",
        "train_ds = train_ds.select([i for i in range(len(train_ds)) if i not in train_empty_audio_indexes_with_inf_loss])\n",
        "\n",
        "train_ds = remove_samples(train_ds)\n",
        "print(train_ds)\n",
        "valid_ds = remove_samples(valid_ds)\n",
        "print(valid_ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3d7cbe70",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([5.638e+03, 1.580e+03, 5.280e+02, 3.461e+03, 4.759e+03, 6.500e+01,\n",
              "        2.700e+01, 1.400e+01, 6.000e+00, 2.000e+00]),\n",
              " array([ 1.63952381,  4.28357143,  6.92761905,  9.57166667, 12.21571429,\n",
              "        14.8597619 , 17.50380952, 20.14785714, 22.79190476, 25.43595238,\n",
              "        28.08      ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pylab as plt\n",
        "plt.hist(train_ds['duration'])\n",
        "plt.hist(valid_ds['duration'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa",
      "metadata": {
        "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa"
      },
      "source": [
        "We'll load the feature extractor from the pre-trained checkpoint with the default values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "802bede2",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'openai/whisper-medium'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x74f5ab8ce4b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/kd/anaconda3/envs/hf2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb",
      "metadata": {
        "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb"
      },
      "source": [
        "### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc82609-a9fb-447a-a2af-99597c864029",
      "metadata": {
        "id": "2bc82609-a9fb-447a-a2af-99597c864029"
      },
      "source": [
        "The Whisper model outputs a sequence of _token ids_. The tokenizer maps each of these token ids to their corresponding text string. For Hindi, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to\n",
        "specify the target language and the task. These arguments inform the\n",
        "tokenizer to prefix the language and task tokens to the start of encoded\n",
        "label sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90d056e20b3e4f14ae0199a1a4ab1bb0",
            "d82a88daec0e4f14add691b7b903064c",
            "350acdb0f40e454099fa901e66de55f0",
            "2e6a82a462cc411d90fa1bea4ee60790",
            "c74bfee0198b4817832ea86e8e88d96c",
            "04fb2d81eff646068e10475a08ae42f4"
          ]
        },
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"Punjabi\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b",
      "metadata": {
        "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
      },
      "source": [
        "### Combine To Create A WhisperProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d",
      "metadata": {
        "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d"
      },
      "source": [
        "To simplify using the feature extractor and tokenizer, we can _wrap_\n",
        "both into a single `WhisperProcessor` class. This processor object\n",
        "inherits from the `WhisperFeatureExtractor` and `WhisperProcessor`,\n",
        "and can be used on the audio inputs and model predictions as required.\n",
        "In doing so, we only need to keep track of two objects during training:\n",
        "the `processor` and the `model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language=\"Punjabi\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f",
      "metadata": {
        "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
      "metadata": {
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': '844424931136176-442-f.m4a.wav', 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00144328,\n",
            "       -0.00148821, -0.0013339 ]), 'sampling_rate': 16000}, 'text': 'à¨‡à¨¨à©à¨¹à¨¾à¨‚ à¨¦à¨¿à¨¨à©€à¨‚ à¨¸à¨¼à©‹à¨… à¨¦à©€ à¨¸à¨¼à©‚à¨Ÿà¨¿à©°à¨— à¨šà©±à¨² à¨°à¨¹à©€ à¨¹à©ˆ à¨…à¨¤à©‡ à¨¸à©ˆà©±à¨Ÿ à¨¤à©‹à¨‚ à¨²à¨¾à¨°à¨¾ à¨¦à©±à¨¤à¨¾ à¨¹à¨¾à¨² à¨¹à©€ à¨µà¨¿à¨š à¨œà¨¾à¨°à©€ à¨¹à©‹à¨ à¨¹à©à¨•à¨®à¨¨à¨¾à¨®à©‡ à¨œà¨¿à¨¸ à¨¦à©‡ à¨µà¨¿à¨š à¨®à©Œà¨œà©‚à¨¦à¨¾ à¨¸à¨•à©±à¨¤à¨° à¨¸ à¨‡à¨¸ à¨²à¨ˆ à¨µà¨°à¨¤à©‡ à¨—à¨ à¨«à¨¾à¨Šà¨‚à¨¡à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨¦à©€à¨†à¨‚ à¨•à¨¿à¨¸à¨®à¨¾à¨‚ à¨¬à¨¹à©à¨¤ à¨¦à¨¿à¨²à¨šà¨¸à¨ª à¨¹à¨¨ à¨†à¨ªà¨£à©‡ à¨¹à©±à¨¥à¨¾à¨‚ à¨¨à©‚à©° à¨¸à¨¼à©€à¨¸à¨¼à©‡ à¨¨à©‚à©° à¨¬à©±à¨¤à©€à¨†à¨‚ à¨œà¨¾à¨‚ à¨¦à©°à¨¦à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨•à©°à¨® à¨•à¨°à¨¨ à¨²à¨ˆ à¨µà¨°à¨¤à©‹ à¨ªà©à¨°à¨•à¨¾à¨¸à¨¼ à¨¸à¨¿à©°à¨˜ à¨¬à¨¾à¨¦à¨² à¨…à¨¤à©‡ à¨…à¨•à¨¾à¨²à©€ à¨¦à¨² à¨¦à©‡ à¨ªà©à¨°à¨§à¨¾à¨¨ à¨…à¨¤à©‡ à¨‰à¨ª à¨®à©à©±à¨– à¨®à©°à¨¤à¨°à©€ à¨¸', 'speaker_id': '442', 'gender': 'f', 'duration': 24.2184126984127, 'source': 'IndicSuperb_pa_ASR__train', 'normalized_text': 'à¨‡à¨¨à©à¨¹à¨¾à¨‚ à¨¦à¨¿à¨¨à©€à¨‚ à¨¸à¨¼à©‹à¨… à¨¦à©€ à¨¸à¨¼à©‚à¨Ÿà¨¿à©°à¨— à¨šà©±à¨² à¨°à¨¹à©€ à¨¹à©ˆ à¨…à¨¤à©‡ à¨¸à©ˆà©±à¨Ÿ à¨¤à©‹à¨‚ à¨²à¨¾à¨°à¨¾ à¨¦à©±à¨¤à¨¾ à¨¹à¨¾à¨² à¨¹à©€ à¨µà¨¿à¨š à¨œà¨¾à¨°à©€ à¨¹à©‹à¨ à¨¹à©à¨•à¨®à¨¨à¨¾à¨®à©‡ à¨œà¨¿à¨¸ à¨¦à©‡ à¨µà¨¿à¨š à¨®à©Œà¨œà©‚à¨¦à¨¾ à¨¸à¨•à©±à¨¤à¨° à¨¸ à¨‡à¨¸ à¨²à¨ˆ à¨µà¨°à¨¤à©‡ à¨—à¨ à¨«à¨¾à¨Šà¨‚à¨¡à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨¦à©€à¨†à¨‚ à¨•à¨¿à¨¸à¨®à¨¾à¨‚ à¨¬à¨¹à©à¨¤ à¨¦à¨¿à¨²à¨šà¨¸à¨ª à¨¹à¨¨ à¨†à¨ªà¨£à©‡ à¨¹à©±à¨¥à¨¾à¨‚ à¨¨à©‚à©° à¨¸à¨¼à©€à¨¸à¨¼à©‡ à¨¨à©‚à©° à¨¬à©±à¨¤à©€à¨†à¨‚ à¨œà¨¾à¨‚ à¨¦à©°à¨¦à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨•à©°à¨® à¨•à¨°à¨¨ à¨²à¨ˆ à¨µà¨°à¨¤à©‹ à¨ªà©à¨°à¨•à¨¾à¨¸à¨¼ à¨¸à¨¿à©°à¨˜ à¨¬à¨¾à¨¦à¨² à¨…à¨¤à©‡ à¨…à¨•à¨¾à¨²à©€ à¨¦à¨² à¨¦à©‡ à¨ªà©à¨°à¨§à¨¾à¨¨ à¨…à¨¤à©‡ à¨‰à¨ª à¨®à©à©±à¨– à¨®à©°à¨¤à¨°à©€ à¨¸'}\n"
          ]
        }
      ],
      "source": [
        "print(ds[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"normalized_text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
      "metadata": {
        "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
      },
      "source": [
        "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f961f425",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
              "        num_rows: 113731\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['audio', 'text', 'speaker_id', 'gender', 'duration', 'source', 'normalized_text'],\n",
              "        num_rows: 16080\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = DatasetDict({\"train\": train_ds, \"valid\": valid_ds})\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [],
      "source": [
        "\n",
        "ds = ds.map(prepare_dataset, remove_columns=ds.column_names[\"train\"], num_proc=1, cache_file_names={'train': \"/mnt/sea/tmp/train-mid.arrow\", 'valid': \"/mnt/sea/tmp/valid-mid.arrow\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
      "metadata": {
        "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15ead5f-2277-4a39-937b-585c2497b2df",
      "metadata": {
        "id": "a15ead5f-2277-4a39-937b-585c2497b2df"
      },
      "source": [
        "We can disable the automatic language detection task performed during inference, and force the model to generate in Hindi. To do so, we set the [langauge](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.generate.language)\n",
        "and [task](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.generate.task)\n",
        "arguments to the generation config. We'll also set any [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)\n",
        "to None, since this was the legacy way of setting the language and\n",
        "task arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
      "metadata": {
        "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
      },
      "outputs": [],
      "source": [
        "model.generation_config.language = \"punjabi\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Define a Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "Let's initialise the data collator we've just defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fee1a7-a44c-461e-b047-c3917221572e",
      "metadata": {
        "id": "66fee1a7-a44c-461e-b047-c3917221572e"
      },
      "source": [
        "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing\n",
        "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from ðŸ¤— Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508",
      "metadata": {
        "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508"
      },
      "source": [
        "We then simply have to define a function that takes our model\n",
        "predictions and returns the WER metric. This function, called\n",
        "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
        "in the `label_ids` (undoing the step we applied in the\n",
        "data collator to ignore padded tokens correctly in the loss).\n",
        "It then decodes the predicted and label ids to strings. Finally,\n",
        "it computes the WER between the predictions and reference labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
      "metadata": {
        "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### Define the Training Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium-pa\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=12,\n",
        "    warmup_steps=1000,\n",
        "    # max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    predict_with_generate=True,\n",
        "    # generation_max_length=225,\n",
        "    save_steps=300,\n",
        "    eval_steps=300,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a944d8-3112-4552-82a0-be25988b3857",
      "metadata": {
        "id": "b3a944d8-3112-4552-82a0-be25988b3857"
      },
      "source": [
        "**Note**: if one does not want to upload the model checkpoints to the Hub,\n",
        "set `push_to_hub=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac29114-d226-4f54-97cf-8718c9f94e1e",
      "metadata": {
        "id": "bac29114-d226-4f54-97cf-8718c9f94e1e"
      },
      "source": [
        "We can forward the training arguments to the ðŸ¤— Trainer along with our model,\n",
        "dataset, data collator and `compute_metrics` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a8513c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 113731\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 16080\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 73310\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 16074\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds)\n",
        "\n",
        "train_labels_lengths = ds['train']['labels']\n",
        "valid_labels_lengths = ds['valid']['labels']\n",
        "\n",
        "train_labels_lengths = [len(i) for i in train_labels_lengths]\n",
        "valid_labels_lengths = [len(i) for i in valid_labels_lengths]\n",
        "\n",
        "train_selected_indexes = [i for i in range(len(train_labels_lengths)) if train_labels_lengths[i] < 448]\n",
        "valid_selected_indexes = [i for i in range(len(valid_labels_lengths)) if valid_labels_lengths[i] < 448]\n",
        "\n",
        "train_ds = ds['train'].select(train_selected_indexes)\n",
        "valid_ds = ds['valid'].select(valid_selected_indexes)\n",
        "\n",
        "ds = DatasetDict({\"train\": train_ds, \"valid\": valid_ds})\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad0ab82",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a30261",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kd/anaconda3/envs/hf2/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "# Test this with IterableDataset\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=ds['train'],\n",
        "    eval_dataset=ds['valid'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uOrRhDGtN5S4",
      "metadata": {
        "id": "uOrRhDGtN5S4"
      },
      "source": [
        "We'll save the processor object once before starting training. Since the processor is not trainable, it won't change over the course of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2zQwMfEOBJq",
      "metadata": {
        "id": "-2zQwMfEOBJq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "481a8643",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_features', 'labels'],\n",
              "    num_rows: 73310\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9b992a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ds['train'][34]['input_features'][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b29298",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "447 9\n"
          ]
        }
      ],
      "source": [
        "labels = ds['train']['labels']\n",
        "len_of_labels = [len(label) for label in labels]\n",
        "max_len = max(len_of_labels)\n",
        "min_len = min(len_of_labels)\n",
        "print(max_len, min_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e8afd71",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# count len_of_labels greater than equals to 448\n",
        "count = 0\n",
        "for i in len_of_labels:\n",
        "    if i >= 448:\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde04dd6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73310"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "113731-40421"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a1e0e6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77f58d4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581f8207",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51575c5a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
      "metadata": {
        "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b584d358",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gc\n",
        "# gc.collect()\n",
        "\n",
        "\n",
        "# # clear gpu ram\n",
        "# torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c9c2181d384c9ebd8eca67c85cd7fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10992 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kd/anaconda3/envs/hf2/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1117, 'grad_norm': 9.816136360168457, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.03}\n",
            "{'loss': 0.9831, 'grad_norm': 8.426244735717773, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.05}\n",
            "{'loss': 0.733, 'grad_norm': 5.270755767822266, 'learning_rate': 7.5e-07, 'epoch': 0.08}\n",
            "{'loss': 0.5062, 'grad_norm': 2.5098249912261963, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.11}\n",
            "{'loss': 0.3632, 'grad_norm': 1.9597983360290527, 'learning_rate': 1.25e-06, 'epoch': 0.14}\n",
            "{'loss': 0.2786, 'grad_norm': 1.7050025463104248, 'learning_rate': 1.5e-06, 'epoch': 0.16}\n",
            "{'loss': 0.2297, 'grad_norm': 3.410609245300293, 'learning_rate': 1.75e-06, 'epoch': 0.19}\n",
            "{'loss': 0.1977, 'grad_norm': 2.1795804500579834, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.22}\n",
            "{'loss': 0.176, 'grad_norm': 1.3645052909851074, 'learning_rate': 2.25e-06, 'epoch': 0.25}\n",
            "{'loss': 0.1609, 'grad_norm': 1.2841603755950928, 'learning_rate': 2.5e-06, 'epoch': 0.27}\n",
            "{'loss': 0.1485, 'grad_norm': 1.5871785879135132, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.3}\n",
            "{'loss': 0.1392, 'grad_norm': 2.787914752960205, 'learning_rate': 3e-06, 'epoch': 0.33}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32a4f102b19843d9beb8c776ccd2f062",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1608 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3",
      "metadata": {
        "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3"
      },
      "source": [
        "Our best WER is 32.0% - not bad for 8h of training data! We can make our model more accessible on the Hub with appropriate tags and README information.\n",
        "You can change these values to match your dataset, language and model\n",
        "name accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper Small Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090d676a-f944-4297-a938-a40eda0b2b68",
      "metadata": {
        "id": "090d676a-f944-4297-a938-a40eda0b2b68"
      },
      "source": [
        "The training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` command and save the preprocessor object we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7030622-caf7-4039-939b-6195cdaa2585",
      "metadata": {
        "id": "d7030622-caf7-4039-939b-6195cdaa2585"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d4360d-5721-426e-b6ac-178f833fedeb",
      "metadata": {
        "id": "34d4360d-5721-426e-b6ac-178f833fedeb"
      },
      "source": [
        "## Building a Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9",
      "metadata": {
        "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9"
      },
      "source": [
        "Now that we've fine-tuned our model we can build a demo to show\n",
        "off its ASR capabilities! We'll make use of ðŸ¤— Transformers\n",
        "`pipeline`, which will take care of the entire ASR pipeline,\n",
        "right from pre-processing the audio inputs to decoding the\n",
        "model predictions.\n",
        "\n",
        "Running the example below will generate a Gradio demo where we\n",
        "can record speech through the microphone of our computer and input it to\n",
        "our fine-tuned Whisper model to transcribe the corresponding text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa",
      "metadata": {
        "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"sanchit-gandhi/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Small Hindi\",\n",
        "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba",
      "metadata": {
        "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba"
      },
      "source": [
        "## Closing Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f737783-2870-4e35-aa11-86a42d7d997a",
      "metadata": {
        "id": "7f737783-2870-4e35-aa11-86a42d7d997a"
      },
      "source": [
        "In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR\n",
        "using ðŸ¤— Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other\n",
        "Transformers models, both for English and multilingual ASR, be sure to check out the\n",
        "examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b3b290",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9b5093f4",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "67e8d4c3",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "87aea5a5",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "175cbd6b",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5b28e408",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b7b004b6",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9fa41b53",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c5dadc8a",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "54e7ec47",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "23ae2e48",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "607465f6",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9c98d9df",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
